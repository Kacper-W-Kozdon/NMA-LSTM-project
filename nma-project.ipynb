{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/kacperkodo/nma-project?scriptVersionId=217827189\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","execution_count":1,"id":"08e6fdf0","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-01-15T20:19:39.044462Z","iopub.status.busy":"2025-01-15T20:19:39.044137Z","iopub.status.idle":"2025-01-15T20:19:39.179069Z","shell.execute_reply":"2025-01-15T20:19:39.178291Z"},"papermill":{"duration":0.145907,"end_time":"2025-01-15T20:19:39.181541","exception":false,"start_time":"2025-01-15T20:19:39.035634","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/medium-articles/medium_articles.csv\n","/kaggle/input/huggingface-bert/bert-base-multilingual-cased/config.json\n","/kaggle/input/huggingface-bert/bert-base-multilingual-cased/tokenizer.json\n","/kaggle/input/huggingface-bert/bert-base-multilingual-cased/tf_model.h5\n","/kaggle/input/huggingface-bert/bert-base-multilingual-cased/pytorch_model.bin\n","/kaggle/input/huggingface-bert/bert-base-multilingual-cased/modelcard.json\n","/kaggle/input/huggingface-bert/bert-base-multilingual-cased/vocab.txt\n","/kaggle/input/huggingface-bert/bert-large-uncased/config.json\n","/kaggle/input/huggingface-bert/bert-large-uncased/tokenizer.json\n","/kaggle/input/huggingface-bert/bert-large-uncased/tf_model.h5\n","/kaggle/input/huggingface-bert/bert-large-uncased/pytorch_model.bin\n","/kaggle/input/huggingface-bert/bert-large-uncased/modelcard.json\n","/kaggle/input/huggingface-bert/bert-large-uncased/vocab.txt\n","/kaggle/input/huggingface-bert/bert-large-uncased/whole-word-masking/._bert_config.json\n","/kaggle/input/huggingface-bert/bert-large-uncased/whole-word-masking/bert_config.json\n","/kaggle/input/huggingface-bert/bert-large-uncased/whole-word-masking/pytorch_model.bin\n","/kaggle/input/huggingface-bert/bert-large-cased/config.json\n","/kaggle/input/huggingface-bert/bert-large-cased/tokenizer.json\n","/kaggle/input/huggingface-bert/bert-large-cased/tf_model.h5\n","/kaggle/input/huggingface-bert/bert-large-cased/pytorch_model.bin\n","/kaggle/input/huggingface-bert/bert-large-cased/modelcard.json\n","/kaggle/input/huggingface-bert/bert-large-cased/vocab.txt\n","/kaggle/input/huggingface-bert/bert-large-cased/whole-word-masking/._bert_config.json\n","/kaggle/input/huggingface-bert/bert-large-cased/whole-word-masking/bert_config.json\n","/kaggle/input/huggingface-bert/bert-large-cased/whole-word-masking/pytorch_model.bin\n","/kaggle/input/huggingface-bert/cl-tohoku/bert-base-japanese-whole-word-masking/config.json\n","/kaggle/input/huggingface-bert/cl-tohoku/bert-base-japanese-whole-word-masking/tf_model.h5\n","/kaggle/input/huggingface-bert/cl-tohoku/bert-base-japanese-whole-word-masking/tokenizer_config.json\n","/kaggle/input/huggingface-bert/cl-tohoku/bert-base-japanese-whole-word-masking/pytorch_model.bin\n","/kaggle/input/huggingface-bert/cl-tohoku/bert-base-japanese-whole-word-masking/vocab.txt\n","/kaggle/input/huggingface-bert/dbmdz/bert-large-cased-finetuned-conll03-english/rust_model.ot\n","/kaggle/input/huggingface-bert/dbmdz/bert-large-cased-finetuned-conll03-english/config.json\n","/kaggle/input/huggingface-bert/dbmdz/bert-large-cased-finetuned-conll03-english/tf_model.h5\n","/kaggle/input/huggingface-bert/dbmdz/bert-large-cased-finetuned-conll03-english/tokenizer_config.json\n","/kaggle/input/huggingface-bert/dbmdz/bert-large-cased-finetuned-conll03-english/pytorch_model.bin\n","/kaggle/input/huggingface-bert/dbmdz/bert-large-cased-finetuned-conll03-english/modelcard.json\n","/kaggle/input/huggingface-bert/dbmdz/bert-large-cased-finetuned-conll03-english/vocab.txt\n","/kaggle/input/huggingface-bert/bert-large-cased-whole-word-masking/config.json\n","/kaggle/input/huggingface-bert/bert-large-cased-whole-word-masking/tokenizer.json\n","/kaggle/input/huggingface-bert/bert-large-cased-whole-word-masking/tf_model.h5\n","/kaggle/input/huggingface-bert/bert-large-cased-whole-word-masking/pytorch_model.bin\n","/kaggle/input/huggingface-bert/bert-large-cased-whole-word-masking/modelcard.json\n","/kaggle/input/huggingface-bert/bert-large-cased-whole-word-masking/vocab.txt\n","/kaggle/input/huggingface-bert/bert-base-cased/config.json\n","/kaggle/input/huggingface-bert/bert-base-cased/tokenizer.json\n","/kaggle/input/huggingface-bert/bert-base-cased/tf_model.h5\n","/kaggle/input/huggingface-bert/bert-base-cased/pytorch_model.bin\n","/kaggle/input/huggingface-bert/bert-base-cased/modelcard.json\n","/kaggle/input/huggingface-bert/bert-base-cased/vocab.txt\n","/kaggle/input/huggingface-bert/bert-base-cased/flax_model.msgpack\n","/kaggle/input/huggingface-bert/bert-base-german-cased/config.json\n","/kaggle/input/huggingface-bert/bert-base-german-cased/tokenizer.json\n","/kaggle/input/huggingface-bert/bert-base-german-cased/tf_model.h5\n","/kaggle/input/huggingface-bert/bert-base-german-cased/pytorch_model.bin\n","/kaggle/input/huggingface-bert/bert-base-german-cased/modelcard.json\n","/kaggle/input/huggingface-bert/bert-base-multilingual-uncased/config.json\n","/kaggle/input/huggingface-bert/bert-base-multilingual-uncased/tokenizer.json\n","/kaggle/input/huggingface-bert/bert-base-multilingual-uncased/tf_model.h5\n","/kaggle/input/huggingface-bert/bert-base-multilingual-uncased/pytorch_model.bin\n","/kaggle/input/huggingface-bert/bert-base-multilingual-uncased/modelcard.json\n","/kaggle/input/huggingface-bert/bert-base-multilingual-uncased/vocab.txt\n","/kaggle/input/huggingface-bert/deepset/bert-large-uncased-whole-word-masking-squad2/config.json\n","/kaggle/input/huggingface-bert/deepset/bert-large-uncased-whole-word-masking-squad2/tokenizer_config.json\n","/kaggle/input/huggingface-bert/deepset/bert-large-uncased-whole-word-masking-squad2/pytorch_model.bin\n","/kaggle/input/huggingface-bert/deepset/bert-large-uncased-whole-word-masking-squad2/modelcard.json\n","/kaggle/input/huggingface-bert/deepset/bert-large-uncased-whole-word-masking-squad2/special_tokens_map.json\n","/kaggle/input/huggingface-bert/deepset/bert-large-uncased-whole-word-masking-squad2/vocab.txt\n","/kaggle/input/huggingface-bert/deepset/bert-large-uncased-whole-word-masking-squad2/added_tokens.json\n","/kaggle/input/huggingface-bert/deepset/bert-large-uncased-whole-word-masking-squad2/saved_model/saved_model.pb\n","/kaggle/input/huggingface-bert/deepset/bert-large-uncased-whole-word-masking-squad2/saved_model/variables/variables.index\n","/kaggle/input/huggingface-bert/deepset/bert-large-uncased-whole-word-masking-squad2/saved_model/variables/variables.data-00000-of-00001\n","/kaggle/input/huggingface-bert/bert-base-chinese/config.json\n","/kaggle/input/huggingface-bert/bert-base-chinese/tokenizer.json\n","/kaggle/input/huggingface-bert/bert-base-chinese/tf_model.h5\n","/kaggle/input/huggingface-bert/bert-base-chinese/pytorch_model.bin\n","/kaggle/input/huggingface-bert/bert-base-chinese/modelcard.json\n","/kaggle/input/huggingface-bert/bert-base-chinese/vocab.txt\n","/kaggle/input/huggingface-bert/bert-base-uncased/rust_model.ot\n","/kaggle/input/huggingface-bert/bert-base-uncased/config.json\n","/kaggle/input/huggingface-bert/bert-base-uncased/tokenizer.json\n","/kaggle/input/huggingface-bert/bert-base-uncased/tf_model.h5\n","/kaggle/input/huggingface-bert/bert-base-uncased/pytorch_model.bin\n","/kaggle/input/huggingface-bert/bert-base-uncased/modelcard.json\n","/kaggle/input/huggingface-bert/bert-base-uncased/vocab.txt\n","/kaggle/input/huggingface-bert/dccuchile/bert-base-spanish-wwm-uncased/config.json\n","/kaggle/input/huggingface-bert/dccuchile/bert-base-spanish-wwm-uncased/tokenizer_config.json\n","/kaggle/input/huggingface-bert/dccuchile/bert-base-spanish-wwm-uncased/pytorch_model.bin\n","/kaggle/input/huggingface-bert/dccuchile/bert-base-spanish-wwm-uncased/special_tokens_map.json\n","/kaggle/input/huggingface-bert/dccuchile/bert-base-spanish-wwm-uncased/vocab.txt\n","/kaggle/input/huggingface-bert/dccuchile/bert-base-spanish-wwm-uncased/added_tokens.json\n","/kaggle/input/huggingface-bert/dccuchile/bert-base-spanish-wwm-cased/config.json\n","/kaggle/input/huggingface-bert/dccuchile/bert-base-spanish-wwm-cased/tokenizer_config.json\n","/kaggle/input/huggingface-bert/dccuchile/bert-base-spanish-wwm-cased/pytorch_model.bin\n","/kaggle/input/huggingface-bert/dccuchile/bert-base-spanish-wwm-cased/special_tokens_map.json\n","/kaggle/input/huggingface-bert/dccuchile/bert-base-spanish-wwm-cased/vocab.txt\n","/kaggle/input/huggingface-bert/dccuchile/bert-base-spanish-wwm-cased/added_tokens.json\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","#!conda install pytorch torchvision -c pytorch\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":2,"id":"cb9b2aff","metadata":{"execution":{"iopub.execute_input":"2025-01-15T20:19:39.194195Z","iopub.status.busy":"2025-01-15T20:19:39.193576Z","iopub.status.idle":"2025-01-15T20:20:21.845096Z","shell.execute_reply":"2025-01-15T20:20:21.844114Z"},"papermill":{"duration":42.665741,"end_time":"2025-01-15T20:20:21.853032","exception":false,"start_time":"2025-01-15T20:19:39.187291","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["0    Photo by Josh Riemer on Unsplash\\n\\nMerry Chri...\n","1    Your Brain On Coronavirus\\n\\nA guide to the cu...\n","2    Mind Your Nose\\n\\nHow smell training can chang...\n","3    Passionate about the synergy between science a...\n","4    You’ve heard of him, haven’t you? Phineas Gage...\n","Name: text, dtype: object"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# necessary libraries\n","#import os\n","import random\n","import pandas as pd \n","#import numpy as np \n","import re\n","from spacy.tokens import Doc\n","import spacy\n","import torch\n","import torch.nn as nn\n","from spacy.tokenizer import Tokenizer\n","from tokenizers import BertWordPieceTokenizer\n","from torch.autograd import Variable\n","import urllib\n","from spacy.tokens import Doc\n","#from spacy.lang.en import English\n","from matplotlib import pyplot as plt  #matplotlib will be helpful to implement later to see if the training progresses.\n","from tensorflow.keras.preprocessing.sequence import pad_sequences   #padding was done in a loop- probably keras would be better for that, but i wasn't sure how it was implemented in it.\n","from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n","#from tensorflow.keras.preprocessing.text import Tokenizer\n","#from tensorflow.keras.models import Sequential\n","#from tensorflow.keras.optimizers import Adam\n","#from tensorflow.keras import regularizers\n","import tensorflow.keras.utils \n","import tensorflow as tf\n","from transformers import AutoTokenizer, AutoModelForMaskedLM, BertTokenizer   #Wasn't used.\n","import gensim\n","from gensim.test.utils import common_texts\n","from gensim.models import KeyedVectors\n","from gensim.models import Word2Vec\n","w2v_model = Word2Vec(sentences = common_texts, vector_size = 64, window = 5, min_count = 1, workers = 4)\n","w2v_model.save(\"/kaggle/working/word2vec.model\")\n","\n","#os.makedirs('../word2vec')\n","\n","MODEL_DIR = \"/kaggle/input/huggingface-bert/\"   #This wasn't used- it's for the vocabulary file for the bert tokenizer- but it has a lot of extra data.\n","#tokenizer = BertTokenizer(MODEL_DIR + \"bert-large-uncased\").encode()\n","#model = AutoModelForMaskedLM.from_pretrained(MODEL_DIR + \"bert-large-uncased\")\n","#model = Word2Vec(sentences, min_count=1)\n","#tokenizer = Word2Vec(sentences, min_count=1)\n","nlp = spacy.load('en_core_web_lg')\n","\n","# loading the dataset\n","nlp_df = pd.read_csv(r'''../input/medium-articles/medium_articles.csv''')[:5]   #Loading first 100 medium articles (it's surprisingly a lot of data).\n","nlp_df.head()\n","nlp_df_texts = nlp_df['text']\n","nlp_df_texts.head()"]},{"cell_type":"code","execution_count":3,"id":"884ec945","metadata":{"execution":{"iopub.execute_input":"2025-01-15T20:20:21.86598Z","iopub.status.busy":"2025-01-15T20:20:21.865722Z","iopub.status.idle":"2025-01-15T20:20:21.874728Z","shell.execute_reply":"2025-01-15T20:20:21.873681Z"},"papermill":{"duration":0.0174,"end_time":"2025-01-15T20:20:21.876406","exception":false,"start_time":"2025-01-15T20:20:21.859006","status":"completed"},"tags":[]},"outputs":[],"source":["#general unit tests\n","#It's does the autotest of functions with the wrapper @autotest, the input is in the commented section- preceeded by \">>> \", the expected output is in the following line.\n","import doctest\n","import copy\n","import functools\n","\n","def autotest(func):\n","    globs = copy.copy(globals())\n","    globs.update({func.__name__: func})\n","    doctest.run_docstring_examples(\n","        func, globs, verbose=True, name=func.__name__)\n","    return func"]},{"cell_type":"code","execution_count":null,"id":"3fbb926a","metadata":{"papermill":{"duration":0.005507,"end_time":"2025-01-15T20:20:21.887586","exception":false,"start_time":"2025-01-15T20:20:21.882079","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":4,"id":"bcfc5ec2","metadata":{"execution":{"iopub.execute_input":"2025-01-15T20:20:21.899846Z","iopub.status.busy":"2025-01-15T20:20:21.899572Z","iopub.status.idle":"2025-01-15T20:20:21.906587Z","shell.execute_reply":"2025-01-15T20:20:21.905863Z"},"papermill":{"duration":0.01512,"end_time":"2025-01-15T20:20:21.908297","exception":false,"start_time":"2025-01-15T20:20:21.893177","status":"completed"},"tags":[]},"outputs":[],"source":["#Copied from the course- setting the random seed.\n","# @title Set random seed\n","\n","# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n","\n","# For DL its critical to set the random seed so that students can have a\n","# baseline to compare their results to expected results.\n","# Read more here: https://pytorch.org/docs/stable/notes/randomness.html\n","\n","# Call `set_seed` function in the exercises to ensure reproducibility.\n","import random\n","import torch\n","\n","def set_seed(seed=None, seed_torch=True):\n","  \"\"\"\n","  Function that controls randomness. NumPy and random modules must be imported.\n","\n","  Args:\n","    seed : Integer\n","      A non-negative integer that defines the random state. Default is `None`.\n","    seed_torch : Boolean\n","      If `True` sets the random seed for pytorch tensors, so pytorch module\n","      must be imported. Default is `True`.\n","\n","  Returns:\n","    Nothing.\n","  \"\"\"\n","  if seed is None:\n","    seed = np.random.choice(2 ** 32)\n","  random.seed(seed)\n","  np.random.seed(seed)\n","  if seed_torch:\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.deterministic = True\n","\n","  print(f'Random seed {seed} has been set.')\n","\n","\n","# In case that `DataLoader` is used\n","def seed_worker(worker_id):\n","  \"\"\"\n","  DataLoader will reseed workers following randomness in\n","  multi-process data loading algorithm.\n","\n","  Args:\n","    worker_id: integer\n","      ID of subprocess to seed. 0 means that\n","      the data will be loaded in the main process\n","      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n","\n","  Returns:\n","    Nothing\n","  \"\"\"\n","  worker_seed = torch.initial_seed() % 2**32\n","  np.random.seed(worker_seed)\n","  random.seed(worker_seed)"]},{"cell_type":"code","execution_count":5,"id":"62486dcd","metadata":{"execution":{"iopub.execute_input":"2025-01-15T20:20:21.920785Z","iopub.status.busy":"2025-01-15T20:20:21.920529Z","iopub.status.idle":"2025-01-15T20:20:21.947969Z","shell.execute_reply":"2025-01-15T20:20:21.946951Z"},"papermill":{"duration":0.035679,"end_time":"2025-01-15T20:20:21.949701","exception":false,"start_time":"2025-01-15T20:20:21.914022","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["It\n","It\n","<class 'str'>\n","<class 'str'>\n","'s\n","'s\n","<class 'str'>\n","<class 'str'>\n","a\n","a\n","<class 'str'>\n","<class 'str'>\n","simple\n","simple\n","<class 'str'>\n","<class 'str'>\n","sentence\n","sentence\n","<class 'str'>\n","<class 'str'>\n",".\n",".\n","<class 'str'>\n","<class 'str'>\n","Second\n","Second\n","<class 'str'>\n","<class 'str'>\n","simple\n","simple\n","<class 'str'>\n","<class 'str'>\n","sentence\n","sentence\n","<class 'str'>\n","<class 'str'>\n"]}],"source":["#Just testing the loading through spacy.\n","doc = nlp(\"It's a simple sentence. Second simple sentence\")\n","for token in doc:\n","    print(token)\n","#    print(type(token))\n","    print(token.text)\n","    print(type(token.text))\n","    token_str = \"\" + token.text\n","    print(type(token_str))"]},{"cell_type":"code","execution_count":6,"id":"9dfd99f3","metadata":{"execution":{"iopub.execute_input":"2025-01-15T20:20:21.962552Z","iopub.status.busy":"2025-01-15T20:20:21.962293Z","iopub.status.idle":"2025-01-15T20:20:21.966936Z","shell.execute_reply":"2025-01-15T20:20:21.966252Z"},"papermill":{"duration":0.01303,"end_time":"2025-01-15T20:20:21.968536","exception":false,"start_time":"2025-01-15T20:20:21.955506","status":"completed"},"tags":[]},"outputs":[],"source":["# @title Set device (GPU or CPU). Execute `set_device()`\n","# especially if torch modules used.\n","\n","# Copied from the course materials.\n","# Inform the user if the notebook uses GPU or CPU.\n","def set_device():\n","    '''\n","    Set the device. CUDA if available, CPU otherwise\n","\n","    Args:\n","    None\n","    \n","    Returns:\n","    Nothing\n","    '''\n","#    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    if torch.cuda.is_available():\n","        device = \"cuda\"\n","    else:\n","        device = \"cpu\"\n","    if device != \"cuda\":\n","        print(\"WARNING: For this notebook to perform best, \"\n","        \"if possible, in the menu under `Runtime` -> \"\n","        \"`Change runtime type.`  select `GPU` \")\n","    else:\n","        print(\"GPU is enabled in this notebook.\")\n","\n","    return device"]},{"cell_type":"code","execution_count":7,"id":"d4062d9f","metadata":{"execution":{"iopub.execute_input":"2025-01-15T20:20:21.981134Z","iopub.status.busy":"2025-01-15T20:20:21.980897Z","iopub.status.idle":"2025-01-15T20:20:22.048877Z","shell.execute_reply":"2025-01-15T20:20:22.047848Z"},"papermill":{"duration":0.076551,"end_time":"2025-01-15T20:20:22.050898","exception":false,"start_time":"2025-01-15T20:20:21.974347","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Random seed 2021 has been set.\n","GPU is enabled in this notebook.\n"]}],"source":["SEED = 2021\n","set_seed(seed=SEED)\n","DEVICE = set_device()"]},{"cell_type":"code","execution_count":8,"id":"74983add","metadata":{"execution":{"iopub.execute_input":"2025-01-15T20:20:22.064354Z","iopub.status.busy":"2025-01-15T20:20:22.064027Z","iopub.status.idle":"2025-01-15T20:20:22.069136Z","shell.execute_reply":"2025-01-15T20:20:22.06815Z"},"papermill":{"duration":0.0137,"end_time":"2025-01-15T20:20:22.070868","exception":false,"start_time":"2025-01-15T20:20:22.057168","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["abcc\n"]}],"source":["#Ruling out some stuff that I thought was causing errors. Ignore this cell.\n","str = 'abc'\n","if not str[-1] == 'b':\n","    str = str + 'c'\n","print(str)"]},{"cell_type":"code","execution_count":9,"id":"db8df6f2","metadata":{"execution":{"iopub.execute_input":"2025-01-15T20:20:22.084293Z","iopub.status.busy":"2025-01-15T20:20:22.084015Z","iopub.status.idle":"2025-01-15T20:20:22.089289Z","shell.execute_reply":"2025-01-15T20:20:22.088382Z"},"papermill":{"duration":0.013702,"end_time":"2025-01-15T20:20:22.091103","exception":false,"start_time":"2025-01-15T20:20:22.077401","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'str'>\n","<class 'list'>\n","<class 'str'> <class 'list'> <class 'list'>\n"]}],"source":["a = \"abc\"\n","b = [\"a\", \"b\", \"c\"]\n","c = [a, b]\n","print(type(\"abc\"))\n","print(type([\"a\", \"b\", \"c\"]))\n","print(type(c[0]), type(c[1]), type(c))\n"]},{"cell_type":"code","execution_count":10,"id":"22019f65","metadata":{"execution":{"iopub.execute_input":"2025-01-15T20:20:22.10405Z","iopub.status.busy":"2025-01-15T20:20:22.103806Z","iopub.status.idle":"2025-01-15T20:20:22.110565Z","shell.execute_reply":"2025-01-15T20:20:22.1099Z"},"papermill":{"duration":0.015143,"end_time":"2025-01-15T20:20:22.112194","exception":false,"start_time":"2025-01-15T20:20:22.097051","status":"completed"},"tags":[]},"outputs":[],"source":["import tempfile\n","#This wasn't helpful and wasn't applied in the end.\n","with tempfile.NamedTemporaryFile(prefix='gensim-model-', delete=False) as tmp:\n","    temporary_filepath = tmp.name\n","    w2v_model.save(temporary_filepath)\n","    #\n","    # The model is now safely stored in the filepath.\n","    # You can copy it to other machines, share it with others, etc.\n","    #\n","    # To load a saved model:\n","    #\n","    new_model = gensim.models.Word2Vec.load(temporary_filepath)"]},{"cell_type":"code","execution_count":11,"id":"28bc7b36","metadata":{"execution":{"iopub.execute_input":"2025-01-15T20:20:22.124936Z","iopub.status.busy":"2025-01-15T20:20:22.124685Z","iopub.status.idle":"2025-01-15T20:20:22.130847Z","shell.execute_reply":"2025-01-15T20:20:22.130086Z"},"papermill":{"duration":0.014503,"end_time":"2025-01-15T20:20:22.132604","exception":false,"start_time":"2025-01-15T20:20:22.118101","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["'Â€'"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["#Checking what sign could've been causing errors in the input. The error was caused by something else. Ignore this cell.\n",">>> '\\x80'.encode().decode('windows-1252')"]},{"cell_type":"code","execution_count":12,"id":"6aee5424","metadata":{"execution":{"iopub.execute_input":"2025-01-15T20:20:22.145841Z","iopub.status.busy":"2025-01-15T20:20:22.145596Z","iopub.status.idle":"2025-01-15T20:20:22.149092Z","shell.execute_reply":"2025-01-15T20:20:22.148372Z"},"papermill":{"duration":0.011933,"end_time":"2025-01-15T20:20:22.150801","exception":false,"start_time":"2025-01-15T20:20:22.138868","status":"completed"},"tags":[]},"outputs":[],"source":["#os.remove(\"/kaggle/working/word2vec.model\")\n","#w2v_model = Word2Vec(sentences = common_texts, vector_size = 100, window = 5, min_count = 1, workers = 4)\n","#w2v_model.save(\"/kaggle/working/word2vec.model\")\n","#DO NOT UNCOMMENT"]},{"cell_type":"code","execution_count":13,"id":"a1b96add","metadata":{"execution":{"iopub.execute_input":"2025-01-15T20:20:22.165138Z","iopub.status.busy":"2025-01-15T20:20:22.164885Z","iopub.status.idle":"2025-01-15T20:20:22.212927Z","shell.execute_reply":"2025-01-15T20:20:22.211912Z"},"papermill":{"duration":0.057352,"end_time":"2025-01-15T20:20:22.214629","exception":false,"start_time":"2025-01-15T20:20:22.157277","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Finding tests in get_sentences\n","Trying:\n","    get_sentences(\"First sentence. Second, sentence.\", autotest = 1)\n","Expecting:\n","    ['First sentence.', 'Second,', 'sentence.']\n","ok\n","Finding tests in train_dictionary\n","Finding tests in tokenizer\n","64\n","[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n","Finding tests in padding\n","Finding tests in sentence_token\n","Finding tests in tokenize_article\n"]}],"source":["#data preprocessing and Tokenize\n","default_length = 64\n","#model = gensim.models.KeyedVectors.load_word2vec_format('/tmp/vectors.txt', binary=False)\n","# using gzipped/bz2 input works too, no need to unzip\n","\n","@autotest\n","def get_sentences(article, autotest = 0, info = 0):\n","    '''\n","    Splits the article into sentences.\n","    \n","    Args: str (list elements from the csv of the dataset)\n","    \n","    Returns: list of str with \".\" being a delimiter.\n","    \n","    >>> get_sentences(\"First sentence. Second, sentence.\", autotest = 1)\n","    ['First sentence.', 'Second,', 'sentence.']\n","    '''\n","    if not autotest:\n","        print(\"Splitting sentences.\") if info else 0\n","    list_of_sentences = []\n","    temp_list_of_sentences = []\n","    for sentence in article.split(\". \"  or \"! \" or \"; \" or \"? \"):   #You could add other delimiters but for the sake of testing only the \". \" was used.\n","        if sentence:\n","            sentence_copy = sentence\n","            if not sentence_copy[-1] == '.':   #.split() method removes the delimiter from the string- I'm adding that back because Word2Vec recognizes punctuation, too.\n","                sentence_copy = sentence_copy + '.'\n","            temp_list_of_sentences.append(sentence_copy)\n","    for sentence in temp_list_of_sentences:   #You could add other delimiters but for the sake of testing only the \". \" was used.\n","        for subsentence in sentence.split(\", \" or \"- \"):\n","             if subsentence:\n","                sentence_copy = subsentence\n","                if not sentence_copy[-1] == ',' and not sentence_copy[-1] == \".\":   #.split() method removes the delimiter from the string- I'm adding that back because Word2Vec recognizes punctuation, too.\n","                    sentence_copy = sentence_copy + ','\n","                if len(sentence_copy) > default_length:\n","                    for i in range(len(sentence_copy) // default_length):    \n","                        list_of_sentences.append(sentence_copy[i * default_length : (i + 1) * default_length - 1])\n","                    list_of_sentences.append(sentence_copy[(len(sentence_copy) // default_length) * default_length : ])\n","                else: \n","                    list_of_sentences.append(sentence_copy)\n","                    \n","#        if len(list_of_sentences) >= 25:\n","#            print(list_of_sentences[-1])\n","    for sentence in list_of_sentences:\n","        sentence.replace(\"\\n\", \" \")\n","        \n","#    if not autotest:\n","#        print(list_of_sentences)\n","    return list_of_sentences\n","\n","@autotest\n","def train_dictionary(list_of_sentences, w2v_model = gensim.models.Word2Vec.load(\"/kaggle/working/word2vec.model\"), autotest = 0, info = 0):\n","    '''\n","    Adds words to the dictionary.\n","    \n","    Args: a list of str, int (for the autotest)\n","    \n","    Returns: None (updates the dictionary file)\n","    \n","    train_dictionary([[\"First\", \"sentence\", \"sentence\"], [0, 0, 0]], autotest = 1)\n","    [-0.00453036  0.11758299  0.00743674  0.22671193  0.02171193 -0.04433674\n","      0.02285469  0.11752532 -0.3050126   0.14662454 -0.19853342 -0.2143049\n","     -0.01861006 -0.07283057 -0.08528131 -0.18882579 -0.09247625  0.00860844\n","     -0.06703243  0.00666352 -0.05344337 -0.07154761 -0.02324829  0.02008165\n","     -0.03207938  0.03745614 -0.04832845 -0.07658006 -0.09805037 -0.08821845\n","      0.23835026  0.1279757   0.07754301 -0.15148504  0.00905246 -0.13989699\n","      0.12004489 -0.29544246 -0.00521144 -0.19496429 -0.06621867 -0.27184018\n","      0.04952901 -0.12884018 -0.152571   -0.19216733  0.11693359 -0.06423832\n","      0.21257243 -0.03538163  0.06508011 -0.23997843 -0.03135304  0.1233076\n","     -0.03994059  0.09518906  0.04058772  0.0460038  -0.12534082  0.10467226\n","      0.12010633  0.06119768 -0.01595154  0.0570786  -0.18848853 -0.08697353\n","     -0.13685709  0.00565706 -0.15765992  0.24080433 -0.0150929  -0.11440682\n","      0.19902213 -0.1956062   0.16332908 -0.046539   -0.05849491  0.23729242\n","      0.02069686  0.06861373 -0.04009774 -0.26628593 -0.2861018   0.07872369\n","     -0.11613636 -0.0366894   0.02865059  0.21707784  0.11488541 -0.01866605\n","      0.15172377 -0.01150565  0.021204    0.14768669  0.24422412  0.03802589\n","     -0.02380455 -0.21103993  0.00822991 -0.11126645]\n","    \n","    '''\n","    #total_examples=model.corpus_count, epochs=model.epochs\n","#    w2v_model = Word2Vec.load(\"word2vec.model\")\n","    \n","#    if autotest:\n","#        print(w2v_model.wv['sentence'])\n","    if not autotest:\n","        print(\"Add words\") if info else 0\n","    w2v_model.build_vocab(list_of_sentences, progress_per=10, update = True)   #Takes a list of sentences (list of str); if the str is long enough, there will be progress showing.  \n","    if not autotest:\n","        print(\"Train Word2Vec\") if info else 0\n","#        print(list_of_sentences)\n","        print(\"Corpus total words: \" + \"{}\".format(w2v_model.corpus_total_words)) if info else 0\n","#    w2v_model.train(list_of_sentences, total_examples = w2v_model.corpus_total_words, epochs = 2)\n","#    w2v_model = Word2Vec(list_of_sentences, min_count=1)\n","#    epochs = w2v_model.epochs\n","#    word_vectors = w2v_model.wv\n","#    word_vectors.save(\"word2vec.wordvectors\")\n","    if not autotest:\n","        print(\"Save\") if info else 0\n","    w2v_model.save(\"/kaggle/working/word2vec.model\")\n","    if not autotest:\n","        print(\"Saved\") if info else 0\n","\n","    # Load back with memory-mapping = read-only, shared across processes.\n","    wv = gensim.models.Word2Vec.load(\"/kaggle/working/word2vec.model\")\n","    if autotest:\n","        print(w2v_model.wv['sentence'])\n","    return w2v_model\n","\n","@autotest\n","def tokenizer(sentences, autotest = 0, info = 0):\n","    '''\n","    Prepares final, vectorized (vectorization happens in a different function) token. It might need some tweaking to make it load unsplit sentences.\n","    \n","    Args: list of str\n","    \n","    Returns: list of lists of str (each str is a word, the inner lists correspond to sentences)\n","    \n","    tokenizer([\"computer\", \"computer\", 0], autotest = 1)\n","    [[-0.005157743580639362, -0.006670279428362846, -0.007779098581522703, 0.00831314641982317, -0.001982920104637742, -0.006856958847492933, -0.0041555981151759624, 0.0051456233486533165, -0.002869971562176943, -0.0037507526576519012, 0.0016218970995396376, -0.0027771019376814365, -0.0015848217299208045, 0.0010748032946139574, -0.0029788126703351736, 0.008521761745214462, 0.003912073094397783, -0.00996176153421402, 0.006261420901864767, -0.006756221409887075, 0.0007696555112488568, 0.004405516665428877, -0.005104861222207546, -0.00211128406226635, 0.008097834885120392, -0.004245028831064701, -0.00763848377391696, 0.009260606952011585, -0.0021561244502663612, -0.004720807541161776, 0.00857329461723566, 0.00428458396345377, 0.004326095338910818, 0.009287215769290924, -0.008455540984869003, 0.005256849806755781, 0.002039944985881448, 0.0041894991882145405, 0.0016983944224193692, 0.004465431906282902, 0.004487594589591026, 0.006106298882514238, -0.0032030295114964247, -0.004577061161398888, -0.00042664064676500857, 0.002534471219405532, -0.0032641179859638214, 0.006059480831027031, 0.004155338276177645, 0.007766852155327797, 0.002570020267739892, 0.00811904389411211, -0.0013876135926693678, 0.008080278523266315, 0.003718096762895584, -0.008049667812883854, -0.003934760577976704, -0.002472597872838378, 0.004894467070698738, -0.0008724128711037338, -0.002831732388585806, 0.007835987024009228, 0.00932561419904232, -0.0016153983306139708, -0.005160751286894083, -0.00470312824472785, -0.004847461357712746, -0.009605620987713337, 0.0013724195305258036, -0.004226145334541798, 0.00252744322642684, 0.0056161172688007355, -0.004067090339958668, -0.009599374607205391, 0.0015471477527171373, -0.006702072452753782, 0.002495900494977832, -0.003781732404604554, 0.0070804813876748085, 0.0006404069717973471, 0.0035619752015918493, -0.0027399309910833836, -0.0017110463231801987, 0.007655020337551832, 0.0014080877881497145, -0.005852151662111282, -0.007836776785552502, 0.0012330447789281607, 0.0064565083011984825, 0.005557966884225607, -0.00897966418415308, 0.008594664745032787, 0.004048154689371586, 0.007471777964383364, 0.009749171324074268, -0.0072917030192911625, -0.009042593650519848, 0.005837699398398399, 0.009393946267664433, 0.0035079459194093943], [-0.005157743580639362, -0.006670279428362846, -0.007779098581522703, 0.00831314641982317, -0.001982920104637742, -0.006856958847492933, -0.0041555981151759624, 0.0051456233486533165, -0.002869971562176943, -0.0037507526576519012, 0.0016218970995396376, -0.0027771019376814365, -0.0015848217299208045, 0.0010748032946139574, -0.0029788126703351736, 0.008521761745214462, 0.003912073094397783, -0.00996176153421402, 0.006261420901864767, -0.006756221409887075, 0.0007696555112488568, 0.004405516665428877, -0.005104861222207546, -0.00211128406226635, 0.008097834885120392, -0.004245028831064701, -0.00763848377391696, 0.009260606952011585, -0.0021561244502663612, -0.004720807541161776, 0.00857329461723566, 0.00428458396345377, 0.004326095338910818, 0.009287215769290924, -0.008455540984869003, 0.005256849806755781, 0.002039944985881448, 0.0041894991882145405, 0.0016983944224193692, 0.004465431906282902, 0.004487594589591026, 0.006106298882514238, -0.0032030295114964247, -0.004577061161398888, -0.00042664064676500857, 0.002534471219405532, -0.0032641179859638214, 0.006059480831027031, 0.004155338276177645, 0.007766852155327797, 0.002570020267739892, 0.00811904389411211, -0.0013876135926693678, 0.008080278523266315, 0.003718096762895584, -0.008049667812883854, -0.003934760577976704, -0.002472597872838378, 0.004894467070698738, -0.0008724128711037338, -0.002831732388585806, 0.007835987024009228, 0.00932561419904232, -0.0016153983306139708, -0.005160751286894083, -0.00470312824472785, -0.004847461357712746, -0.009605620987713337, 0.0013724195305258036, -0.004226145334541798, 0.00252744322642684, 0.0056161172688007355, -0.004067090339958668, -0.009599374607205391, 0.0015471477527171373, -0.006702072452753782, 0.002495900494977832, -0.003781732404604554, 0.0070804813876748085, 0.0006404069717973471, 0.0035619752015918493, -0.0027399309910833836, -0.0017110463231801987, 0.007655020337551832, 0.0014080877881497145, -0.005852151662111282, -0.007836776785552502, 0.0012330447789281607, 0.0064565083011984825, 0.005557966884225607, -0.00897966418415308, 0.008594664745032787, 0.004048154689371586, 0.007471777964383364, 0.009749171324074268, -0.0072917030192911625, -0.009042593650519848, 0.005837699398398399, 0.009393946267664433, 0.0035079459194093943], [-0.0005362272495403886, 0.00023643016174901277, 0.005103349685668945, 0.009009272791445255, -0.009302949532866478, -0.007116809021681547, 0.006458871532231569, 0.008972988463938236, -0.005015428178012371, -0.0037633730098605156, 0.007380504626780748, -0.0015334725612774491, -0.004536614287644625, 0.006554050371050835, -0.004860160406678915, -0.0018160176696255803, 0.002876579761505127, 0.000991873792372644, -0.008285215124487877, -0.009448818862438202, 0.0073117660358548164, 0.005070262122899294, 0.006757693365216255, 0.0007628655293956399, 0.006350889336317778, -0.0034053658600896597, -0.0009464025497436523, 0.005768573377281427, -0.00752163864672184, -0.003936104942113161, -0.007511582225561142, -0.0009300422389060259, 0.009538118727505207, -0.007319166790693998, -0.0023337698075920343, -0.0019377422286197543, 0.008077435195446014, -0.005930895917117596, 4.516124681686051e-05, -0.004753734916448593, -0.009603550657629967, 0.005007293075323105, -0.008759587071835995, -0.004391825292259455, -3.5099983506370336e-05, -0.00029618263943120837, -0.007661240175366402, 0.009614741429686546, 0.004982056561857462, 0.00923314318060875, -0.008157918229699135, 0.004495797213166952, -0.004137077368795872, 0.000824534916318953, 0.008498618379235268, -0.004462177865207195, 0.004517500288784504, -0.0067869615741074085, -0.0035484887193888426, 0.0093985078856349, -0.0015776539221405983, 0.00032137156813405454, -0.0041406298987567425, -0.007682688068598509, -0.0015080093871802092, 0.002469794824719429, -0.0008880281238816679, 0.0055336616933345795, -0.002742977114394307, 0.002260065171867609, 0.005455794278532267, 0.008345952257514, -0.001453740638680756, -0.009208142757415771, 0.004370551090687513, 0.0005717849708162248, 0.007441906724125147, -0.000813283899333328, -0.002638413803651929, -0.008753009140491486, -0.0008565568714402616, 0.0028265619184821844, 0.005401427857577801, 0.007052655331790447, -0.005703122820705175, 0.0018588185776025057, 0.006088862195611, -0.004798052366822958, -0.003107261611148715, 0.006797628477215767, 0.0016314744716510177, 0.00018991708930116147, 0.0034736371599137783, 0.00021777629444841295, 0.00961882621049881, 0.005060603842139244, -0.008917391300201416, -0.007041561417281628, 0.0009014558745548129, 0.006392533890902996]]\n","    '''\n","#    if not autotest:\n","#        print(\"Creating vectorized tokens.\")\n","    if type(sentences) == type(\"string\"):\n","        sentences = [sentences]\n","    words_vector = []\n","    for word in sentences:\n","        if not autotest:\n","            print(word) if info else 0\n","        print(type(word), word, word in w2v_model.wv.key_to_index.keys()) if info else 0\n","        if word in w2v_model.wv.key_to_index.keys():\n","            vector = w2v_model.wv[word]\n","            print(vector) if info else 0\n","            vector = vector.tolist()\n","            print(vector) if info else 0\n","            words_vector.append(vector)\n","#    if not autotest:\n","        print(words_vector[0]) if info else 0\n","        print(\"\\n\\n\\n\") if info else 0\n","    print(len(words_vector[0])) if info else 0\n","    return words_vector\n","\n","#print(tokenizer([\"computer\"]))\n","padding_length = len(tokenizer([\"computer\"])[0])\n","print(padding_length)\n","\n","print(np.zeros((1, padding_length)).tolist())\n","            \n","@autotest\n","def padding(sentence, max_sentence_length = default_length, labels = 0, info = 0):\n","    '''\n","    Fills in the spots in tokens with zeros until all the tokens have an equal length (default_length).\n","    padding(['It', \"'s\", 'a', 'simple', 'sentence', '.'])\n","    ['It', \"'s\", 'a', 'simple', 'sentence', '.', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","    '''\n","#    print(sentence.shape[0] < max(default_length, max_sentence_length))\n","#    while sentence.shape[0] < max(default_length, max_sentence_length):\n","#        if labels:\n","#            sentence[-1] = 0\n","#        sentence = np.append(sentence, [0])\n","    while len(sentence) < default_length:\n","        if labels:   #Conditional to remove one word from the sentence for the training set (replace with 0).\n","            sentence[-1] = (0 * np.array(sentence[0])).tolist()\n","#        print(sentence)\n","#        sentence += [(0 * np.array(sentence[0])).tolist()]\n","        sentence += np.zeros((1, padding_length)).tolist()\n","#        sentence[-1] = int(sentence[-1])\n","#        print(sentence)\n","    print(sentence) if info else 0\n","    return sentence\n","        \n","@autotest\n","def sentence_token(sentence, vectorized = 1, labels = 0, autotest = 0, info = 0, pad = 1):\n","    '''\n","    Creating tokens for each sentence.\n","    \n","    Args: str, int (0 or 1 whether to apply word2vec), int (0 or 1 whether it's for training or labels)\n","    \n","    Returns: a list of str\n","    \n","    sentence_token(\"It's a simple sentence.\", 0, autotest = 1)\n","    ['It', \"'s\", 'a', 'simple', 'sentence', '.', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","    '''\n","    if not autotest:\n","        print(\"Creating sentence tokens.\") if info else 0\n","#        print(sentence)\n","    if vectorized:\n","        sentence_token = []\n","    else:\n","        sentence_token = []\n","    doc = nlp(sentence)\n","    print(type(doc)) if info else 0\n","    index = 0\n","    for token in doc:\n","#        if vectorized:\n","#            sentence_token = torch.cat((sentence_token, torch.tensor([[Tokenizer(token)]])), dim = 1)\n","#        else:\n","#            sentence_token = torch.cat((sentence_token, token), dim = 1)\n","        if index == 0:\n","            print(tokenizer(token.text, autotest)) if info else 0\n","            print(token.text) if info else 0\n","        index += 1\n","        if vectorized:\n","            sentence_token += tokenizer(token.text, autotest)\n","#            sentence_token.append(tokenizer(token.text, autotest))\n","\n","        else:\n","            sentence_token.append(token.text)\n","    \n","#    print(sentence_token.shape[0])\n","    if pad:\n","        sentence_token = padding(sentence_token, labels)\n","#    print(padding(sentence_token, training = 0 ))\n","    \n","    print(sentence_token) if info else 0\n","#    if vectorized:\n","#        flattened_token = torch.flatten(torch.tensor(sentence_token), start_dim = 0)    #Flattening to feature vectores of the size of 10000x1 (from 100x100) per sentence\n","#        print(flattened_token.shape) if info else 0\n","#        flattened_token = flattened_token.tolist()\n","#    else:\n","#        flattened_token = sentence_token\n","    \n","    return sentence_token\n","    \n","@autotest\n","def tokenize_article(article, vectorized = 1, labels = 0, autotest = 0, info = 0):\n","    '''\n","    Creates a list of lists of tokens of the article. The last sentence of the article retains a dot.\n","    \n","    Args: str from the dataset (a whole article), int (vectorization), int (training/labels)\n","    \n","    Returns: a list of lists containing str\n","    \n","    tokenize_article(\"First sentence. Second sentence.\", 0, autotest = 1)\n","    [['First', 'sentence', '.', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], ['Second', 'sentence', '.', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n","    '''\n","    sentence_token_list = []\n","    for sentence in get_sentences(article, autotest):\n","#        if not autotest:\n","        print(sentence) if info else 0\n","#            print(sentence_token(sentence,vectorized, labels))\n","        sentence_token_list += [sentence_token(sentence,vectorized, labels, autotest, pad = 1)]\n","        print(sentence_token_list) if info else 0\n","#    if not autotest:\n","#        print(\"Article tokenized.\")\n","#    print(torch.tensor(sentence_token_list).shape) if not info else 0\n","    return sentence_token_list"]},{"cell_type":"code","execution_count":null,"id":"0c210802","metadata":{"papermill":{"duration":0.005842,"end_time":"2025-01-15T20:20:22.226592","exception":false,"start_time":"2025-01-15T20:20:22.22075","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"7d0543ac","metadata":{"papermill":{"duration":0.005911,"end_time":"2025-01-15T20:20:22.238458","exception":false,"start_time":"2025-01-15T20:20:22.232547","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":14,"id":"549f3d01","metadata":{"execution":{"iopub.execute_input":"2025-01-15T20:20:22.251483Z","iopub.status.busy":"2025-01-15T20:20:22.251181Z","iopub.status.idle":"2025-01-15T20:20:22.255126Z","shell.execute_reply":"2025-01-15T20:20:22.254462Z"},"papermill":{"duration":0.012335,"end_time":"2025-01-15T20:20:22.256751","exception":false,"start_time":"2025-01-15T20:20:22.244416","status":"completed"},"tags":[]},"outputs":[],"source":["w2v_model.wv.key_to_index.keys();"]},{"cell_type":"code","execution_count":15,"id":"e1579fb8","metadata":{"execution":{"iopub.execute_input":"2025-01-15T20:20:22.269819Z","iopub.status.busy":"2025-01-15T20:20:22.269572Z","iopub.status.idle":"2025-01-15T20:20:25.769466Z","shell.execute_reply":"2025-01-15T20:20:25.768545Z"},"papermill":{"duration":3.508634,"end_time":"2025-01-15T20:20:25.771339","exception":false,"start_time":"2025-01-15T20:20:22.262705","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Vocabulary count after training step: 1443\r"]}],"source":["dataset = nlp_df_texts   \n","def create_vocab(dataset, vectorized = 1, labels = 0, autotest = 0, info = 0):\n","    '''\n","    Split into sentences -> create sentence tokens -> apply word2vec to sentence tokens of the form: [[\"str\", \"str\", ..., \".\"], [\"str\", \"str\", ..., \".\"], ...]\n","    \n","    Args: str (entire dataset)\n","    \n","    Returns: None (updates the vocabulary)\n","    '''\n","    w2v_model = gensim.models.Word2Vec.load(\"/kaggle/working/word2vec.model\")\n","    for article in dataset:\n","        vocab_len = len(w2v_model.wv)\n","#        print(\"Vocabulary count before training step: \" + \"{}\".format(vocab_len))\n","        w2v_model = gensim.models.Word2Vec.load(\"/kaggle/working/word2vec.model\")\n","    \n","#        list_of_sentences = tokenize_article(article, 0)\n","        sentences = get_sentences(article)\n","        list_of_sentences = []\n","        for sentence in sentences:\n","            list_of_sentences += [sentence_token(sentence, vectorized = 0, labels = 0, autotest = 0, info = 0, pad = 0)]\n","#        print(list_of_sentences)\n","#        print(list_of_sentences)\n","        w2v_model = train_dictionary(list_of_sentences)\n","        w2v_model.save(\"/kaggle/working/word2vec.model\")\n","        w2v_model = gensim.models.Word2Vec.load(\"/kaggle/working/word2vec.model\")\n","        vocab_len = len(w2v_model.wv)\n","        print(\"Vocabulary count after training step: \" + \"{}\".format(vocab_len), end = \"\\r\")\n","#    w2v_model = KeyedVectors.load(\"word2vec.model\", mmap='r')    \n","    print(w2v_model.wv.key_to_index.keys()) if info else 0\n","    return w2v_model\n","create_vocab(dataset, info = 0)  #Starts vocabulary training\n","w2v_model = gensim.models.Word2Vec.load(\"/kaggle/working/word2vec.model\")\n","#w2v_model.wv.key_to_index.keys()"]},{"cell_type":"code","execution_count":16,"id":"c8fd0981","metadata":{"execution":{"iopub.execute_input":"2025-01-15T20:20:25.784881Z","iopub.status.busy":"2025-01-15T20:20:25.784608Z","iopub.status.idle":"2025-01-15T20:20:25.813561Z","shell.execute_reply":"2025-01-15T20:20:25.812671Z"},"papermill":{"duration":0.037479,"end_time":"2025-01-15T20:20:25.815226","exception":false,"start_time":"2025-01-15T20:20:25.777747","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["['simple computer.', 'very simple.']\n","simple computer.\n","torch.Size([64, 64])\n","torch.Size([2, 64, 64])\n"]},{"data":{"text/plain":["torch.Size([2, 4096])"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["string = \"simple computer. very simple\"\n","tempsentences = get_sentences(string)\n","print(tempsentences)\n","print(tempsentences[0])\n","toks = sentence_token(tempsentences[0], vectorized = 1)\n","#print(tokens)\n","#print(tokenizer([\"simple\"]))\n","tok = tokenize_article(string)\n","print(torch.tensor(toks).shape)\n","print(torch.tensor(tok).shape)\n","t = torch.tensor(tok)    #Need to flatten sentences in vectorized form, so each sentence goes from 100 x 100 to 10000 x 1.\n","#print(t.shape)\n","flattened = torch.flatten(t, start_dim = 1)\n","flattened.shape"]},{"cell_type":"code","execution_count":17,"id":"cf383203","metadata":{"execution":{"iopub.execute_input":"2025-01-15T20:20:25.828549Z","iopub.status.busy":"2025-01-15T20:20:25.828292Z","iopub.status.idle":"2025-01-15T20:20:25.846011Z","shell.execute_reply":"2025-01-15T20:20:25.845346Z"},"papermill":{"duration":0.026495,"end_time":"2025-01-15T20:20:25.847861","exception":false,"start_time":"2025-01-15T20:20:25.821366","status":"completed"},"tags":[]},"outputs":[],"source":["w2v_model = gensim.models.Word2Vec.load(\"/kaggle/working/word2vec.model\")\n","w2v_model.wv.key_to_index.keys();"]},{"cell_type":"code","execution_count":18,"id":"d472d424","metadata":{"execution":{"iopub.execute_input":"2025-01-15T20:20:25.861493Z","iopub.status.busy":"2025-01-15T20:20:25.861237Z","iopub.status.idle":"2025-01-15T20:20:25.898504Z","shell.execute_reply":"2025-01-15T20:20:25.897826Z"},"papermill":{"duration":0.04611,"end_time":"2025-01-15T20:20:25.900153","exception":false,"start_time":"2025-01-15T20:20:25.854043","status":"completed"},"tags":[]},"outputs":[],"source":["# LSTM() returns tuple of (tensor, (recurrent state))\n","w2v_length = 64\n","class extract_tensor(nn.Module):\n","    def forward(self,x):\n","        # Output shape (batch, features, hidden)\n","        tensor, _ = x\n","        # Reshape shape (batch, hidden)\n","        return tensor[:, -1, :]\n","\n","\n","#RNN (LSTM)\n","dataset = nlp_df_texts\n","model = Word2Vec.load(\"word2vec.model\")\n","class RNN_LSTM(nn.Module):\n","    def __init__(self, embedding_dim, hidden_dim, n_layers = 1, lr = 0.003, batches = 0):\n","        super(RNN_LSTM, self).__init__()\n","        self.embedding_dim = embedding_dim   #the dimensions of the input.\n","        self.hidden_dim = hidden_dim   #Width of the model.\n","        self.batches = batches    #Number of examples\n","        self.num_layers = n_layers   #Depth of the model.\n","        self.lr = lr   #Learning rate\n","        self.h_0 = torch.nn.Parameter(torch.rand(self.num_layers * 2, self.batches, int(0.5 * np.floor(self.hidden_dim)), requires_grad = True))\n","        self.c_0 = torch.nn.Parameter(torch.rand(self.num_layers * 2, self.batches, int(1 * np.floor(self.hidden_dim)), requires_grad = True))\n","        print(self.embedding_dim, self.hidden_dim, self.num_layers, self.lr)\n","        # Next line is for setting up the neural net, LSTM is the RNN variant (bidirectional).\n","        self.modnlp = nn.Sequential(\n","            nn.LSTM(input_size = w2v_length, hidden_size = self.hidden_dim, proj_size = int(0.5 * np.floor(self.hidden_dim)), num_layers = self.num_layers, batch_first = True, dropout = 0.3, bidirectional = True),\n","            extract_tensor(),\n","            nn.Linear(in_features = self.hidden_dim , out_features = int(np.floor(self.embedding_dim)), bias = True , device = None , dtype = None ),\n","            nn.Softmax(dim = 1)\n","#            self.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","        )\n","        \n","    \n","        \n","    def load_article(self, article, vectorized = 1, labels = 0):   #Helper function.\n","#        print(article)\n","        loaded_article = torch.tensor(tokenize_article(article, vectorized, labels))\n","#        print(loaded_article)\n","#        print(loaded_article)\n","        self.max_sentence_length = lambda max_sentence_length: max(article.map(len))\n","#        print(tokenize_article(article, vectorized, labels))\n","        return loaded_article\n","    \n","    def load_all_articles(self, dataset, vectorized = 1):   #Loading the dataset into the object.\n","        training_dataset = []\n","        labels_dataset = []\n","        index = 1\n","        for article in dataset:\n","            print(\"Article \" + f\"{index}\", end = \"\\r\")\n","#            print(article)\n","#            print(self.load_article(article, vectorized = 1, labels = 0))\n","            training_dataset += self.load_article(article, vectorized = 1, labels = 0)\n","            labels_dataset += self.load_article(article, vectorized = 1, labels = 1)\n","#            print(torch.tensor(training_dataset).shape)\n","            index += 1\n","        print(labels_dataset[0].shape, len(labels_dataset))\n","#        print(training_dataset)\n","        no_batches = len(training_dataset)\n","        print(no_batches)\n","        return (training_dataset, labels_dataset, no_batches)\n","    \n","    def forward_(self, training_dataset, labels_dataset, epochs = 5):   #Training step.\n","        linear = nn.Linear(in_features = self.hidden_dim , out_features = int(np.floor(0.5 * self.embedding_dim)), bias = True , device = None , dtype = None )\n","        lstm = nn.LSTM(input_size = w2v_length, hidden_size = self.hidden_dim, proj_size = int(np.floor(0.5 * self.hidden_dim)), num_layers = self.num_layers, batch_first = True, dropout = 0.3, bidirectional = True)\n","        softmax = nn.Softmax(dim = 1)\n","        training_dataset = torch.tensor(training_dataset)\n","        labels_dataset = torch.tensor(labels_dataset)\n","        print(training_dataset.shape)\n","        print(type(training_dataset))\n","        loss_function = nn.CrossEntropyLoss()\n","        optimizer = torch.optim.Adam(self.parameters(), self.lr)\n","#        print(self.parameters())\n","        h_0 = self.h_0\n","        c_0 = self.c_0\n","        h_0.requires_grad_(True)\n","        c_0.requires_grad_(True)\n","        logits = torch.tensor(torch.zeros(1))\n","        logits.requires_grad_(True)\n","        self.train()\n","        for epoch in range(epochs):\n","            for sentence_idx, sentence in enumerate(training_dataset):\n","                optimizer.zero_grad()\n","                logits, (h_n, c_n) = lstm(sentence.reshape(sentence.shape[0], 1, -1), (h_0, c_0))\n","    #            print(torch.tensor(logits).shape)\n","                logits = linear(logits)\n","                logits = softmax(logits)\n","                logits = torch.tensor(logits).requires_grad_(True)\n","    #            print(torch.tensor(logits).shape)\n","                logits_labels = labels_dataset[sentence_idx].reshape(sentence.shape[0], -1)\n","                # logits_labels = linear(labels_dataset[sentence_idx].reshape(sentence.shape[0], 1, -1))\n","                # logits_labels = softmax(logits_labels)\n","                # logits_labels = torch.tensor(logits_labels)\n","    #            print(soft_logits[0] - soft_logits_labels[0])\n","                print(logits.shape)\n","                loss = loss_function(logits, logits_labels)\n","                loss.requires_grad_(True)\n","                loss.retain_grad()\n","                loss.backward(retain_graph = True)\n","                optimizer.step()\n","            print(loss.item(), \"epoch\" + f\"{epoch}\")\n","#            print(logits.grad)\n","        self.eval()\n","        return self.parameters, loss\n","        #self.modnlp(training_dataset, labels_dataset)\n","        \n","#        print(self.model.summary())\n","\n","    def train_lstm(self, training_dataset, labels_dataset, epochs = 5):   #Training step.\n","        training_dataset = training_dataset\n","        labels_dataset = labels_dataset\n","        print(len(training_dataset))\n","        print(type(training_dataset))\n","        loss_function = nn.CrossEntropyLoss()\n","        optimizer = torch.optim.Adam(self.parameters(), self.lr)\n","#        print(self.parameters())\n","        h_0 = self.h_0\n","        c_0 = self.c_0\n","        h_0.requires_grad_(True)\n","        c_0.requires_grad_(True)\n","        logits = torch.tensor(torch.zeros(1))\n","        logits.requires_grad_(True)\n","        self.train()\n","        for epoch in range(epochs):\n","            for sentence_idx, sentence in enumerate(training_dataset):\n","                optimizer.zero_grad()\n","                logits = self.modnlp(sentence.reshape(sentence.shape[0], 1, -1)).requires_grad_(True)\n","    #            print(torch.tensor(logits).shape)\n","                # logits_labels = self.modnlp(labels_dataset[sentence_idx]sentence.shape[0], 1, -1))\n","                logits_labels = labels_dataset[sentence_idx].reshape(sentence.shape[0], -1)\n","    #            print(soft_logits[0] - soft_logits_labels[0])\n","                # print(f\"OUT SHAPE: {logits.shape}, LABELS SHAPE: {logits_labels.shape}\")\n","                loss = loss_function(logits, logits_labels)\n","                loss.requires_grad_(True)\n","                loss.retain_grad()\n","                loss.backward(retain_graph = True)\n","                optimizer.step()\n","            print(loss.item(), \"epoch\" + f\"{epoch}\")\n","#            print(logits.grad)\n","        self.eval()\n","        return self\n","\n","    def forward(self, sentence):\n","        self.eval()\n","        vec = self.modnlp(sentence.reshape(sentence.shape[0], 1, -1))\n","        out = w2v_model.wv.similar_by_vector(vec[0].detach().numpy())\n","        return out\n","        "]},{"cell_type":"code","execution_count":19,"id":"56a2ac0f","metadata":{"execution":{"iopub.execute_input":"2025-01-15T20:20:25.913532Z","iopub.status.busy":"2025-01-15T20:20:25.913279Z","iopub.status.idle":"2025-01-15T20:20:25.917597Z","shell.execute_reply":"2025-01-15T20:20:25.916854Z"},"papermill":{"duration":0.013097,"end_time":"2025-01-15T20:20:25.919519","exception":false,"start_time":"2025-01-15T20:20:25.906422","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["0    Photo by Josh Riemer on Unsplash\\n\\nMerry Chri...\n","1    Your Brain On Coronavirus\\n\\nA guide to the cu...\n","2    Mind Your Nose\\n\\nHow smell training can chang...\n","3    Passionate about the synergy between science a...\n","4    You’ve heard of him, haven’t you? Phineas Gage...\n","Name: text, dtype: object\n"]}],"source":["print(dataset)\n","#test_article = tokenize_article(dataset[0], vectorized = 1, labels = 0);\n","#test_article2 = test_article[0]\n","#print(len(test_article[:][:][0][:]))\n","#print(test_article[:][:][0][:])"]},{"cell_type":"code","execution_count":20,"id":"df586db1","metadata":{"execution":{"iopub.execute_input":"2025-01-15T20:20:25.93289Z","iopub.status.busy":"2025-01-15T20:20:25.932655Z","iopub.status.idle":"2025-01-15T20:20:32.894474Z","shell.execute_reply":"2025-01-15T20:20:32.893394Z"},"papermill":{"duration":6.970709,"end_time":"2025-01-15T20:20:32.896432","exception":false,"start_time":"2025-01-15T20:20:25.925723","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["64 64 2 0.003\n","torch.Size([64, 64]) 477\n","477\n"]}],"source":["training_dataset, labels_dataset, no_batches = RNN_LSTM(default_length, hidden_dim = w2v_length, n_layers = 2, lr = 0.003).load_all_articles(dataset)    #Loads the dataset."]},{"cell_type":"code","execution_count":21,"id":"cdbde801","metadata":{"execution":{"iopub.execute_input":"2025-01-15T20:20:32.910876Z","iopub.status.busy":"2025-01-15T20:20:32.910589Z","iopub.status.idle":"2025-01-15T20:20:32.915235Z","shell.execute_reply":"2025-01-15T20:20:32.914356Z"},"papermill":{"duration":0.01381,"end_time":"2025-01-15T20:20:32.91706","exception":false,"start_time":"2025-01-15T20:20:32.90325","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([64, 64])\n"]}],"source":["print(training_dataset[1].shape)"]},{"cell_type":"code","execution_count":22,"id":"3c62355a","metadata":{"execution":{"iopub.execute_input":"2025-01-15T20:20:32.931063Z","iopub.status.busy":"2025-01-15T20:20:32.930806Z","iopub.status.idle":"2025-01-15T20:20:43.375731Z","shell.execute_reply":"2025-01-15T20:20:43.374785Z"},"papermill":{"duration":10.454163,"end_time":"2025-01-15T20:20:43.377784","exception":false,"start_time":"2025-01-15T20:20:32.923621","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["64 64 2 0.003\n","477\n","<class 'list'>\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:121: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"]},{"name":"stdout","output_type":"stream","text":["-0.0076605710200965405 epoch0\n","-0.007660570554435253 epoch1\n","-0.0076605710200965405 epoch2\n","-0.0076605710200965405 epoch3\n","-0.0076605710200965405 epoch4\n"]}],"source":["text_generator = RNN_LSTM(default_length, hidden_dim = w2v_length, batches = no_batches, n_layers = 2, lr = 0.003).train_lstm(training_dataset, labels_dataset, epochs = 5)   #Runs the model."]},{"cell_type":"code","execution_count":23,"id":"4241d831","metadata":{"execution":{"iopub.execute_input":"2025-01-15T20:20:43.39286Z","iopub.status.busy":"2025-01-15T20:20:43.392561Z","iopub.status.idle":"2025-01-15T20:20:43.408638Z","shell.execute_reply":"2025-01-15T20:20:43.406105Z"},"papermill":{"duration":0.026622,"end_time":"2025-01-15T20:20:43.411569","exception":false,"start_time":"2025-01-15T20:20:43.384947","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[-1.4592e-02,  5.3236e-03,  4.1692e-04,  6.0287e-03,  1.1544e-02,\n","         -1.0511e-02,  8.7283e-03, -1.4883e-02, -1.2573e-03, -1.3580e-02,\n","         -7.9690e-03,  1.4519e-02, -2.9044e-03,  4.5551e-03,  1.4178e-02,\n","          1.3970e-02, -1.2829e-02, -4.7081e-03,  1.5452e-02,  7.9780e-03,\n","         -2.4821e-03, -1.3585e-02,  4.6287e-03, -1.0434e-02,  1.2710e-02,\n","         -6.9666e-03, -1.6698e-03,  1.5729e-03, -2.9870e-04,  1.7945e-03,\n","          9.5557e-03, -3.1685e-05, -5.0733e-03, -2.3612e-03,  9.2172e-03,\n","          2.3665e-03, -1.1320e-03,  1.4587e-02, -7.6917e-03, -1.3104e-03,\n","          1.4341e-02,  1.0549e-02,  2.3489e-03, -1.3883e-02,  1.7954e-03,\n","         -3.5764e-03,  1.4642e-02,  1.8911e-03,  2.3289e-03,  3.7611e-03,\n","         -2.8696e-03, -7.8142e-03,  3.6328e-04, -3.1481e-03,  1.0317e-02,\n","          1.3973e-02, -1.0546e-03,  4.6529e-03, -9.5460e-03,  2.6560e-03,\n","         -1.0825e-02, -1.3588e-02, -9.2218e-03, -1.3999e-02]])\n","<class 'numpy.ndarray'>\n","('smells', 0.23284420371055603)\n"]}],"source":["word = torch.tensor(tokenizer(\"computer\"))\n","print(word)\n","print(type(w2v_model.wv[\"like\"]))\n","print(text_generator.forward(word)[0])\n"]},{"cell_type":"code","execution_count":24,"id":"e4c29f78","metadata":{"execution":{"iopub.execute_input":"2025-01-15T20:20:43.453913Z","iopub.status.busy":"2025-01-15T20:20:43.452841Z","iopub.status.idle":"2025-01-15T20:20:43.460578Z","shell.execute_reply":"2025-01-15T20:20:43.458611Z"},"papermill":{"duration":0.03427,"end_time":"2025-01-15T20:20:43.464378","exception":false,"start_time":"2025-01-15T20:20:43.430108","status":"completed"},"tags":[]},"outputs":[],"source":["#if nlp_df:\n","#    del nlp_df"]},{"cell_type":"code","execution_count":null,"id":"c12ab49e","metadata":{"execution":{"iopub.status.busy":"2025-01-15T20:12:05.894519Z","iopub.status.idle":"2025-01-15T20:12:05.897847Z","shell.execute_reply":"2025-01-15T20:12:05.89753Z","shell.execute_reply.started":"2025-01-15T20:12:05.897495Z"},"papermill":{"duration":0.020082,"end_time":"2025-01-15T20:20:43.504913","exception":false,"start_time":"2025-01-15T20:20:43.484831","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":25,"id":"aad89d59","metadata":{"execution":{"iopub.execute_input":"2025-01-15T20:20:43.540297Z","iopub.status.busy":"2025-01-15T20:20:43.540028Z","iopub.status.idle":"2025-01-15T20:20:43.545723Z","shell.execute_reply":"2025-01-15T20:20:43.544485Z"},"papermill":{"duration":0.023762,"end_time":"2025-01-15T20:20:43.54937","exception":false,"start_time":"2025-01-15T20:20:43.525608","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["['__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__ignoreds', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__numpys', '__recursive_saveloads', '__reduce__', '__reduce_ex__', '__repr__', '__scipys', '__setattr__', '__setitem__', '__sizeof__', '__slotnames__', '__str__', '__subclasshook__', '__weakref__', '_adapt_by_suffix', '_load_specials', '_log_evaluate_word_analogies', '_save_specials', '_smart_save', '_upconvert_old_d2vkv', '_upconvert_old_vocab', 'add_lifecycle_event', 'add_vector', 'add_vectors', 'allocate_vecattrs', 'closer_than', 'cosine_similarities', 'distance', 'distances', 'doesnt_match', 'evaluate_word_analogies', 'evaluate_word_pairs', 'expandos', 'fill_norms', 'get_index', 'get_normed_vectors', 'get_vecattr', 'get_vector', 'has_index_for', 'index2entity', 'index2word', 'index_to_key', 'init_sims', 'intersect_word2vec_format', 'key_to_index', 'load', 'load_word2vec_format', 'log_accuracy', 'log_evaluate_word_pairs', 'mapfile_path', 'most_similar', 'most_similar_cosmul', 'most_similar_to_given', 'n_similarity', 'next_index', 'norms', 'rank', 'rank_by_centrality', 'relative_cosine_similarity', 'resize_vectors', 'save', 'save_word2vec_format', 'set_vecattr', 'similar_by_key', 'similar_by_vector', 'similar_by_word', 'similarity', 'similarity_unseen_docs', 'sort_by_descending_frequency', 'unit_normalize_all', 'vector_size', 'vectors', 'vectors_lockf', 'vectors_norm', 'vocab', 'wmdistance', 'word_vec', 'words_closer_than']\n","[('like', 1.0), ('namely', 0.4298384189605713), ('happen', 0.4029092788696289), ('had', 0.3717750608921051), ('next', 0.3384186029434204), ('transplant', 0.335035502910614), ('look', 0.3285600543022156), ('psych', 0.32095035910606384), ('could', 0.3067287802696228), ('role', 0.30269524455070496)]\n"]}],"source":["print(dir(w2v_model.wv))\n","print(w2v_model.wv.similar_by_vector(w2v_model.wv[\"like\"]))\n"]},{"cell_type":"code","execution_count":null,"id":"c436d2a5","metadata":{"papermill":{"duration":0.020487,"end_time":"2025-01-15T20:20:43.590622","exception":false,"start_time":"2025-01-15T20:20:43.570135","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"0a727152","metadata":{"papermill":{"duration":0.020371,"end_time":"2025-01-15T20:20:43.631577","exception":false,"start_time":"2025-01-15T20:20:43.611206","status":"completed"},"tags":[]},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":2123951,"sourceId":3531703,"sourceType":"datasetVersion"},{"datasetId":934701,"sourceId":10407136,"sourceType":"datasetVersion"}],"dockerImageVersionId":30207,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":75.330106,"end_time":"2025-01-15T20:20:47.175956","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-01-15T20:19:31.84585","version":"2.3.4"}},"nbformat":4,"nbformat_minor":5}