{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n#!conda install pytorch torchvision -c pytorch\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-09-02T15:38:30.445037Z","iopub.execute_input":"2022-09-02T15:38:30.445603Z","iopub.status.idle":"2022-09-02T15:38:30.499492Z","shell.execute_reply.started":"2022-09-02T15:38:30.445567Z","shell.execute_reply":"2022-09-02T15:38:30.497934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# necessary libraries\n#import os\nimport random\nimport pandas as pd \n#import numpy as np \nimport re\nfrom spacy.tokens import Doc\nimport spacy\nimport torch\nimport torch.nn as nn\nfrom spacy.tokenizer import Tokenizer\nfrom tokenizers import BertWordPieceTokenizer\nfrom torch.autograd import Variable\nimport urllib\nfrom spacy.tokens import Doc\n#from spacy.lang.en import English\nfrom matplotlib import pyplot as plt  #matplotlib will be helpful to implement later to see if the training progresses.\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences   #padding was done in a loop- probably keras would be better for that, but i wasn't sure how it was implemented in it.\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n#from tensorflow.keras.preprocessing.text import Tokenizer\n#from tensorflow.keras.models import Sequential\n#from tensorflow.keras.optimizers import Adam\n#from tensorflow.keras import regularizers\nimport tensorflow.keras.utils \nimport tensorflow as tf\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM, BertTokenizer   #Wasn't used.\nimport gensim\nfrom gensim.test.utils import common_texts\nfrom gensim.models import KeyedVectors\nfrom gensim.models import Word2Vec\nw2v_model = Word2Vec(sentences = common_texts, vector_size = 100, window = 5, min_count = 1, workers = 4)\nw2v_model.save(\"/kaggle/working/word2vec.model\")\n\n#os.makedirs('../word2vec')\n\nMODEL_DIR = \"/kaggle/input/huggingface-bert/\"   #This wasn't used- it's for the vocabulary file for the bert tokenizer- but it has a lot of extra data.\n#tokenizer = BertTokenizer(MODEL_DIR + \"bert-large-uncased\").encode()\n#model = AutoModelForMaskedLM.from_pretrained(MODEL_DIR + \"bert-large-uncased\")\n#model = Word2Vec(sentences, min_count=1)\n#tokenizer = Word2Vec(sentences, min_count=1)\nnlp = spacy.load('en_core_web_lg')\n\n# loading the dataset\nnlp_df = pd.read_csv(r'''../input/medium-articles/medium_articles.csv''')[:25]   #Loading first 100 medium articles (it's surprisingly a lot of data).\nnlp_df.head()\nnlp_df_texts = nlp_df['text']\nnlp_df_texts.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-02T15:38:30.502939Z","iopub.execute_input":"2022-09-02T15:38:30.50398Z","iopub.status.idle":"2022-09-02T15:38:51.875616Z","shell.execute_reply.started":"2022-09-02T15:38:30.503917Z","shell.execute_reply":"2022-09-02T15:38:51.872434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#general unit tests\n#It's does the autotest of functions with the wrapper @autotest, the input is in the commented section- preceeded by \">>> \", the expected output is in the following line.\nimport doctest\nimport copy\nimport functools\n\ndef autotest(func):\n    globs = copy.copy(globals())\n    globs.update({func.__name__: func})\n    doctest.run_docstring_examples(\n        func, globs, verbose=True, name=func.__name__)\n    return func","metadata":{"execution":{"iopub.status.busy":"2022-09-02T15:38:51.87791Z","iopub.execute_input":"2022-09-02T15:38:51.87878Z","iopub.status.idle":"2022-09-02T15:38:51.88971Z","shell.execute_reply.started":"2022-09-02T15:38:51.878725Z","shell.execute_reply":"2022-09-02T15:38:51.887926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Copied from the course- setting the random seed.\n# @title Set random seed\n\n# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n\n# For DL its critical to set the random seed so that students can have a\n# baseline to compare their results to expected results.\n# Read more here: https://pytorch.org/docs/stable/notes/randomness.html\n\n# Call `set_seed` function in the exercises to ensure reproducibility.\nimport random\nimport torch\n\ndef set_seed(seed=None, seed_torch=True):\n  \"\"\"\n  Function that controls randomness. NumPy and random modules must be imported.\n\n  Args:\n    seed : Integer\n      A non-negative integer that defines the random state. Default is `None`.\n    seed_torch : Boolean\n      If `True` sets the random seed for pytorch tensors, so pytorch module\n      must be imported. Default is `True`.\n\n  Returns:\n    Nothing.\n  \"\"\"\n  if seed is None:\n    seed = np.random.choice(2 ** 32)\n  random.seed(seed)\n  np.random.seed(seed)\n  if seed_torch:\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\n  print(f'Random seed {seed} has been set.')\n\n\n# In case that `DataLoader` is used\ndef seed_worker(worker_id):\n  \"\"\"\n  DataLoader will reseed workers following randomness in\n  multi-process data loading algorithm.\n\n  Args:\n    worker_id: integer\n      ID of subprocess to seed. 0 means that\n      the data will be loaded in the main process\n      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n\n  Returns:\n    Nothing\n  \"\"\"\n  worker_seed = torch.initial_seed() % 2**32\n  np.random.seed(worker_seed)\n  random.seed(worker_seed)","metadata":{"execution":{"iopub.status.busy":"2022-09-02T15:38:51.895396Z","iopub.execute_input":"2022-09-02T15:38:51.895974Z","iopub.status.idle":"2022-09-02T15:38:51.907653Z","shell.execute_reply.started":"2022-09-02T15:38:51.895942Z","shell.execute_reply":"2022-09-02T15:38:51.906316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Just testing the loading through spacy.\ndoc = nlp(\"It's a simple sentence. Second simple sentence\")\nfor token in doc:\n    print(token)\n#    print(type(token))\n    print(token.text)\n    print(type(token.text))\n    token_str = \"\" + token.text\n    print(type(token_str))","metadata":{"execution":{"iopub.status.busy":"2022-09-02T15:38:51.909771Z","iopub.execute_input":"2022-09-02T15:38:51.910708Z","iopub.status.idle":"2022-09-02T15:38:52.694555Z","shell.execute_reply.started":"2022-09-02T15:38:51.910661Z","shell.execute_reply":"2022-09-02T15:38:52.692123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# @title Set device (GPU or CPU). Execute `set_device()`\n# especially if torch modules used.\n\n# Copied from the course materials.\n# Inform the user if the notebook uses GPU or CPU.\ndef set_device():\n    '''\n    Set the device. CUDA if available, CPU otherwise\n\n    Args:\n    None\n    \n    Returns:\n    Nothing\n    '''\n#    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    if torch.cuda.is_available():\n        device = \"cuda\"\n    else:\n        device = \"cpu\"\n    if device != \"cuda\":\n        print(\"WARNING: For this notebook to perform best, \"\n        \"if possible, in the menu under `Runtime` -> \"\n        \"`Change runtime type.`  select `GPU` \")\n    else:\n        print(\"GPU is enabled in this notebook.\")\n\n    return device","metadata":{"execution":{"iopub.status.busy":"2022-09-02T15:38:52.706782Z","iopub.execute_input":"2022-09-02T15:38:52.707159Z","iopub.status.idle":"2022-09-02T15:38:52.714682Z","shell.execute_reply.started":"2022-09-02T15:38:52.707125Z","shell.execute_reply":"2022-09-02T15:38:52.713196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = 2021\nset_seed(seed=SEED)\nDEVICE = set_device()","metadata":{"execution":{"iopub.status.busy":"2022-09-02T15:38:52.716723Z","iopub.execute_input":"2022-09-02T15:38:52.717609Z","iopub.status.idle":"2022-09-02T15:38:52.811227Z","shell.execute_reply.started":"2022-09-02T15:38:52.717564Z","shell.execute_reply":"2022-09-02T15:38:52.809558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Ruling out some stuff that I thought was causing errors. Ignore this cell.\nstr = 'abc'\nif not str[-1] == 'b':\n    str = str + 'c'\nprint(str)","metadata":{"execution":{"iopub.status.busy":"2022-09-02T15:38:52.813726Z","iopub.execute_input":"2022-09-02T15:38:52.814779Z","iopub.status.idle":"2022-09-02T15:38:52.823363Z","shell.execute_reply.started":"2022-09-02T15:38:52.814716Z","shell.execute_reply":"2022-09-02T15:38:52.821689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = \"abc\"\nb = [\"a\", \"b\", \"c\"]\nc = [a, b]\nprint(type(\"abc\"))\nprint(type([\"a\", \"b\", \"c\"]))\nprint(type(c[0]), type(c[1]), type(c))\n","metadata":{"execution":{"iopub.status.busy":"2022-09-02T15:38:52.825914Z","iopub.execute_input":"2022-09-02T15:38:52.82695Z","iopub.status.idle":"2022-09-02T15:38:52.838111Z","shell.execute_reply.started":"2022-09-02T15:38:52.826904Z","shell.execute_reply":"2022-09-02T15:38:52.836207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tempfile\n#This wasn't helpful and wasn't applied in the end.\nwith tempfile.NamedTemporaryFile(prefix='gensim-model-', delete=False) as tmp:\n    temporary_filepath = tmp.name\n    w2v_model.save(temporary_filepath)\n    #\n    # The model is now safely stored in the filepath.\n    # You can copy it to other machines, share it with others, etc.\n    #\n    # To load a saved model:\n    #\n    new_model = gensim.models.Word2Vec.load(temporary_filepath)","metadata":{"execution":{"iopub.status.busy":"2022-09-02T15:38:52.846396Z","iopub.execute_input":"2022-09-02T15:38:52.846759Z","iopub.status.idle":"2022-09-02T15:38:52.860587Z","shell.execute_reply.started":"2022-09-02T15:38:52.84673Z","shell.execute_reply":"2022-09-02T15:38:52.858803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking what sign could've been causing errors in the input. The error was caused by something else. Ignore this cell.\n>>> '\\x80'.encode().decode('windows-1252')","metadata":{"execution":{"iopub.status.busy":"2022-09-02T15:38:52.864019Z","iopub.execute_input":"2022-09-02T15:38:52.864985Z","iopub.status.idle":"2022-09-02T15:38:52.878905Z","shell.execute_reply.started":"2022-09-02T15:38:52.864939Z","shell.execute_reply":"2022-09-02T15:38:52.877343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#os.remove(\"/kaggle/working/word2vec.model\")\n#w2v_model = Word2Vec(sentences = common_texts, vector_size = 100, window = 5, min_count = 1, workers = 4)\n#w2v_model.save(\"/kaggle/working/word2vec.model\")\n#DO NOT UNCOMMENT","metadata":{"execution":{"iopub.status.busy":"2022-09-02T15:38:52.881299Z","iopub.execute_input":"2022-09-02T15:38:52.882579Z","iopub.status.idle":"2022-09-02T15:38:52.892663Z","shell.execute_reply.started":"2022-09-02T15:38:52.882436Z","shell.execute_reply":"2022-09-02T15:38:52.89116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#data preprocessing and Tokenize\ndefault_length = 50\n#model = gensim.models.KeyedVectors.load_word2vec_format('/tmp/vectors.txt', binary=False)\n# using gzipped/bz2 input works too, no need to unzip\n\n@autotest\ndef get_sentences(article, autotest = 0, info = 0):\n    '''\n    Splits the article into sentences.\n    \n    Args: str (list elements from the csv of the dataset)\n    \n    Returns: list of str with \".\" being a delimiter.\n    \n    >>> get_sentences(\"First sentence. Second, sentence.\", autotest = 1)\n    ['First sentence.', 'Second,', 'sentence.']\n    '''\n    if not autotest:\n        print(\"Splitting sentences.\") if info else 0\n    list_of_sentences = []\n    temp_list_of_sentences = []\n    for sentence in article.split(\". \"  or \"! \" or \"; \" or \"? \"):   #You could add other delimiters but for the sake of testing only the \". \" was used.\n        if sentence:\n            sentence_copy = sentence\n            if not sentence_copy[-1] == '.':   #.split() method removes the delimiter from the string- I'm adding that back because Word2Vec recognizes punctuation, too.\n                sentence_copy = sentence_copy + '.'\n            temp_list_of_sentences.append(sentence_copy)\n    for sentence in temp_list_of_sentences:   #You could add other delimiters but for the sake of testing only the \". \" was used.\n        for subsentence in sentence.split(\", \" or \"- \"):\n             if subsentence:\n                sentence_copy = subsentence\n                if not sentence_copy[-1] == ',' and not sentence_copy[-1] == \".\":   #.split() method removes the delimiter from the string- I'm adding that back because Word2Vec recognizes punctuation, too.\n                    sentence_copy = sentence_copy + ','\n                if len(sentence_copy) > default_length:\n                    for i in range(len(sentence_copy) // default_length):    \n                        list_of_sentences.append(sentence_copy[i * default_length : (i + 1) * default_length - 1])\n                    list_of_sentences.append(sentence_copy[(len(sentence_copy) // default_length) * default_length : ])\n                else: \n                    list_of_sentences.append(sentence_copy)\n                    \n#        if len(list_of_sentences) >= 25:\n#            print(list_of_sentences[-1])\n    for sentence in list_of_sentences:\n        sentence.replace(\"\\n\", \" \")\n        \n#    if not autotest:\n#        print(list_of_sentences)\n    return list_of_sentences\n\n@autotest\ndef train_dictionary(list_of_sentences, w2v_model = gensim.models.Word2Vec.load(\"/kaggle/working/word2vec.model\"), autotest = 0, info = 0):\n    '''\n    Adds words to the dictionary.\n    \n    Args: a list of str, int (for the autotest)\n    \n    Returns: None (updates the dictionary file)\n    \n    train_dictionary([[\"First\", \"sentence\", \"sentence\"], [0, 0, 0]], autotest = 1)\n    [-0.00453036  0.11758299  0.00743674  0.22671193  0.02171193 -0.04433674\n      0.02285469  0.11752532 -0.3050126   0.14662454 -0.19853342 -0.2143049\n     -0.01861006 -0.07283057 -0.08528131 -0.18882579 -0.09247625  0.00860844\n     -0.06703243  0.00666352 -0.05344337 -0.07154761 -0.02324829  0.02008165\n     -0.03207938  0.03745614 -0.04832845 -0.07658006 -0.09805037 -0.08821845\n      0.23835026  0.1279757   0.07754301 -0.15148504  0.00905246 -0.13989699\n      0.12004489 -0.29544246 -0.00521144 -0.19496429 -0.06621867 -0.27184018\n      0.04952901 -0.12884018 -0.152571   -0.19216733  0.11693359 -0.06423832\n      0.21257243 -0.03538163  0.06508011 -0.23997843 -0.03135304  0.1233076\n     -0.03994059  0.09518906  0.04058772  0.0460038  -0.12534082  0.10467226\n      0.12010633  0.06119768 -0.01595154  0.0570786  -0.18848853 -0.08697353\n     -0.13685709  0.00565706 -0.15765992  0.24080433 -0.0150929  -0.11440682\n      0.19902213 -0.1956062   0.16332908 -0.046539   -0.05849491  0.23729242\n      0.02069686  0.06861373 -0.04009774 -0.26628593 -0.2861018   0.07872369\n     -0.11613636 -0.0366894   0.02865059  0.21707784  0.11488541 -0.01866605\n      0.15172377 -0.01150565  0.021204    0.14768669  0.24422412  0.03802589\n     -0.02380455 -0.21103993  0.00822991 -0.11126645]\n    \n    '''\n    #total_examples=model.corpus_count, epochs=model.epochs\n#    w2v_model = Word2Vec.load(\"word2vec.model\")\n    \n#    if autotest:\n#        print(w2v_model.wv['sentence'])\n    if not autotest:\n        print(\"Add words\") if info else 0\n    w2v_model.build_vocab(list_of_sentences, progress_per=10, update = True)   #Takes a list of sentences (list of str); if the str is long enough, there will be progress showing.  \n    if not autotest:\n        print(\"Train Word2Vec\") if info else 0\n#        print(list_of_sentences)\n        print(\"Corpus total words: \" + \"{}\".format(w2v_model.corpus_total_words)) if info else 0\n#    w2v_model.train(list_of_sentences, total_examples = w2v_model.corpus_total_words, epochs = 2)\n#    w2v_model = Word2Vec(list_of_sentences, min_count=1)\n#    epochs = w2v_model.epochs\n#    word_vectors = w2v_model.wv\n#    word_vectors.save(\"word2vec.wordvectors\")\n    if not autotest:\n        print(\"Save\") if info else 0\n    w2v_model.save(\"/kaggle/working/word2vec.model\")\n    if not autotest:\n        print(\"Saved\") if info else 0\n\n    # Load back with memory-mapping = read-only, shared across processes.\n    wv = gensim.models.Word2Vec.load(\"/kaggle/working/word2vec.model\")\n    if autotest:\n        print(w2v_model.wv['sentence'])\n    return w2v_model\n\n@autotest\ndef tokenizer(sentences, autotest = 0, info = 0):\n    '''\n    Prepares final, vectorized (vectorization happens in a different function) token. It might need some tweaking to make it load unsplit sentences.\n    \n    Args: list of str\n    \n    Returns: list of lists of str (each str is a word, the inner lists correspond to sentences)\n    \n    tokenizer([\"computer\", \"computer\", 0], autotest = 1)\n    [[-0.005157743580639362, -0.006670279428362846, -0.007779098581522703, 0.00831314641982317, -0.001982920104637742, -0.006856958847492933, -0.0041555981151759624, 0.0051456233486533165, -0.002869971562176943, -0.0037507526576519012, 0.0016218970995396376, -0.0027771019376814365, -0.0015848217299208045, 0.0010748032946139574, -0.0029788126703351736, 0.008521761745214462, 0.003912073094397783, -0.00996176153421402, 0.006261420901864767, -0.006756221409887075, 0.0007696555112488568, 0.004405516665428877, -0.005104861222207546, -0.00211128406226635, 0.008097834885120392, -0.004245028831064701, -0.00763848377391696, 0.009260606952011585, -0.0021561244502663612, -0.004720807541161776, 0.00857329461723566, 0.00428458396345377, 0.004326095338910818, 0.009287215769290924, -0.008455540984869003, 0.005256849806755781, 0.002039944985881448, 0.0041894991882145405, 0.0016983944224193692, 0.004465431906282902, 0.004487594589591026, 0.006106298882514238, -0.0032030295114964247, -0.004577061161398888, -0.00042664064676500857, 0.002534471219405532, -0.0032641179859638214, 0.006059480831027031, 0.004155338276177645, 0.007766852155327797, 0.002570020267739892, 0.00811904389411211, -0.0013876135926693678, 0.008080278523266315, 0.003718096762895584, -0.008049667812883854, -0.003934760577976704, -0.002472597872838378, 0.004894467070698738, -0.0008724128711037338, -0.002831732388585806, 0.007835987024009228, 0.00932561419904232, -0.0016153983306139708, -0.005160751286894083, -0.00470312824472785, -0.004847461357712746, -0.009605620987713337, 0.0013724195305258036, -0.004226145334541798, 0.00252744322642684, 0.0056161172688007355, -0.004067090339958668, -0.009599374607205391, 0.0015471477527171373, -0.006702072452753782, 0.002495900494977832, -0.003781732404604554, 0.0070804813876748085, 0.0006404069717973471, 0.0035619752015918493, -0.0027399309910833836, -0.0017110463231801987, 0.007655020337551832, 0.0014080877881497145, -0.005852151662111282, -0.007836776785552502, 0.0012330447789281607, 0.0064565083011984825, 0.005557966884225607, -0.00897966418415308, 0.008594664745032787, 0.004048154689371586, 0.007471777964383364, 0.009749171324074268, -0.0072917030192911625, -0.009042593650519848, 0.005837699398398399, 0.009393946267664433, 0.0035079459194093943], [-0.005157743580639362, -0.006670279428362846, -0.007779098581522703, 0.00831314641982317, -0.001982920104637742, -0.006856958847492933, -0.0041555981151759624, 0.0051456233486533165, -0.002869971562176943, -0.0037507526576519012, 0.0016218970995396376, -0.0027771019376814365, -0.0015848217299208045, 0.0010748032946139574, -0.0029788126703351736, 0.008521761745214462, 0.003912073094397783, -0.00996176153421402, 0.006261420901864767, -0.006756221409887075, 0.0007696555112488568, 0.004405516665428877, -0.005104861222207546, -0.00211128406226635, 0.008097834885120392, -0.004245028831064701, -0.00763848377391696, 0.009260606952011585, -0.0021561244502663612, -0.004720807541161776, 0.00857329461723566, 0.00428458396345377, 0.004326095338910818, 0.009287215769290924, -0.008455540984869003, 0.005256849806755781, 0.002039944985881448, 0.0041894991882145405, 0.0016983944224193692, 0.004465431906282902, 0.004487594589591026, 0.006106298882514238, -0.0032030295114964247, -0.004577061161398888, -0.00042664064676500857, 0.002534471219405532, -0.0032641179859638214, 0.006059480831027031, 0.004155338276177645, 0.007766852155327797, 0.002570020267739892, 0.00811904389411211, -0.0013876135926693678, 0.008080278523266315, 0.003718096762895584, -0.008049667812883854, -0.003934760577976704, -0.002472597872838378, 0.004894467070698738, -0.0008724128711037338, -0.002831732388585806, 0.007835987024009228, 0.00932561419904232, -0.0016153983306139708, -0.005160751286894083, -0.00470312824472785, -0.004847461357712746, -0.009605620987713337, 0.0013724195305258036, -0.004226145334541798, 0.00252744322642684, 0.0056161172688007355, -0.004067090339958668, -0.009599374607205391, 0.0015471477527171373, -0.006702072452753782, 0.002495900494977832, -0.003781732404604554, 0.0070804813876748085, 0.0006404069717973471, 0.0035619752015918493, -0.0027399309910833836, -0.0017110463231801987, 0.007655020337551832, 0.0014080877881497145, -0.005852151662111282, -0.007836776785552502, 0.0012330447789281607, 0.0064565083011984825, 0.005557966884225607, -0.00897966418415308, 0.008594664745032787, 0.004048154689371586, 0.007471777964383364, 0.009749171324074268, -0.0072917030192911625, -0.009042593650519848, 0.005837699398398399, 0.009393946267664433, 0.0035079459194093943], [-0.0005362272495403886, 0.00023643016174901277, 0.005103349685668945, 0.009009272791445255, -0.009302949532866478, -0.007116809021681547, 0.006458871532231569, 0.008972988463938236, -0.005015428178012371, -0.0037633730098605156, 0.007380504626780748, -0.0015334725612774491, -0.004536614287644625, 0.006554050371050835, -0.004860160406678915, -0.0018160176696255803, 0.002876579761505127, 0.000991873792372644, -0.008285215124487877, -0.009448818862438202, 0.0073117660358548164, 0.005070262122899294, 0.006757693365216255, 0.0007628655293956399, 0.006350889336317778, -0.0034053658600896597, -0.0009464025497436523, 0.005768573377281427, -0.00752163864672184, -0.003936104942113161, -0.007511582225561142, -0.0009300422389060259, 0.009538118727505207, -0.007319166790693998, -0.0023337698075920343, -0.0019377422286197543, 0.008077435195446014, -0.005930895917117596, 4.516124681686051e-05, -0.004753734916448593, -0.009603550657629967, 0.005007293075323105, -0.008759587071835995, -0.004391825292259455, -3.5099983506370336e-05, -0.00029618263943120837, -0.007661240175366402, 0.009614741429686546, 0.004982056561857462, 0.00923314318060875, -0.008157918229699135, 0.004495797213166952, -0.004137077368795872, 0.000824534916318953, 0.008498618379235268, -0.004462177865207195, 0.004517500288784504, -0.0067869615741074085, -0.0035484887193888426, 0.0093985078856349, -0.0015776539221405983, 0.00032137156813405454, -0.0041406298987567425, -0.007682688068598509, -0.0015080093871802092, 0.002469794824719429, -0.0008880281238816679, 0.0055336616933345795, -0.002742977114394307, 0.002260065171867609, 0.005455794278532267, 0.008345952257514, -0.001453740638680756, -0.009208142757415771, 0.004370551090687513, 0.0005717849708162248, 0.007441906724125147, -0.000813283899333328, -0.002638413803651929, -0.008753009140491486, -0.0008565568714402616, 0.0028265619184821844, 0.005401427857577801, 0.007052655331790447, -0.005703122820705175, 0.0018588185776025057, 0.006088862195611, -0.004798052366822958, -0.003107261611148715, 0.006797628477215767, 0.0016314744716510177, 0.00018991708930116147, 0.0034736371599137783, 0.00021777629444841295, 0.00961882621049881, 0.005060603842139244, -0.008917391300201416, -0.007041561417281628, 0.0009014558745548129, 0.006392533890902996]]\n    '''\n#    if not autotest:\n#        print(\"Creating vectorized tokens.\")\n    if type(sentences) == type(\"string\"):\n        sentences = [sentences]\n    words_vector = []\n    for word in sentences:\n        if not autotest:\n            print(word) if info else 0\n        print(type(word), word, word in w2v_model.wv.key_to_index.keys()) if info else 0\n        if word in w2v_model.wv.key_to_index.keys():\n            vector = w2v_model.wv[word]\n            print(vector) if info else 0\n            vector = vector.tolist()\n            print(vector) if info else 0\n            words_vector.append(vector)\n#    if not autotest:\n        print(words_vector[0]) if info else 0\n        print(\"\\n\\n\\n\") if info else 0\n    print(len(words_vector[0])) if info else 0\n    return words_vector\n\n#print(tokenizer([\"computer\"]))\npadding_length = len(tokenizer([\"computer\"])[0])\nprint(padding_length)\n\nprint(np.zeros((1, padding_length)).tolist())\n            \n@autotest\ndef padding(sentence, max_sentence_length = default_length, labels = 0, info = 0):\n    '''\n    Fills in the spots in tokens with zeros until all the tokens have an equal length (default_length).\n    padding(['It', \"'s\", 'a', 'simple', 'sentence', '.'])\n    ['It', \"'s\", 'a', 'simple', 'sentence', '.', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n    '''\n#    print(sentence.shape[0] < max(default_length, max_sentence_length))\n#    while sentence.shape[0] < max(default_length, max_sentence_length):\n#        if labels:\n#            sentence[-1] = 0\n#        sentence = np.append(sentence, [0])\n    while len(sentence) < default_length:\n        if labels:   #Conditional to remove one word from the sentence for the training set (replace with 0).\n            sentence[-1] = (0 * np.array(sentence[0])).tolist()\n#        print(sentence)\n#        sentence += [(0 * np.array(sentence[0])).tolist()]\n        sentence += np.zeros((1, padding_length)).tolist()\n#        sentence[-1] = int(sentence[-1])\n#        print(sentence)\n    print(sentence) if info else 0\n    return sentence\n        \n@autotest\ndef sentence_token(sentence, vectorized = 1, labels = 0, autotest = 0, info = 0, pad = 1):\n    '''\n    Creating tokens for each sentence.\n    \n    Args: str, int (0 or 1 whether to apply word2vec), int (0 or 1 whether it's for training or labels)\n    \n    Returns: a list of str\n    \n    sentence_token(\"It's a simple sentence.\", 0, autotest = 1)\n    ['It', \"'s\", 'a', 'simple', 'sentence', '.', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n    '''\n    if not autotest:\n        print(\"Creating sentence tokens.\") if info else 0\n#        print(sentence)\n    if vectorized:\n        sentence_token = []\n    else:\n        sentence_token = []\n    doc = nlp(sentence)\n    print(type(doc)) if info else 0\n    index = 0\n    for token in doc:\n#        if vectorized:\n#            sentence_token = torch.cat((sentence_token, torch.tensor([[Tokenizer(token)]])), dim = 1)\n#        else:\n#            sentence_token = torch.cat((sentence_token, token), dim = 1)\n        if index == 0:\n            print(tokenizer(token.text, autotest)) if info else 0\n            print(token.text) if info else 0\n        index += 1\n        if vectorized:\n            sentence_token += tokenizer(token.text, autotest)\n#            sentence_token.append(tokenizer(token.text, autotest))\n\n        else:\n            sentence_token.append(token.text)\n    \n#    print(sentence_token.shape[0])\n    if pad:\n        sentence_token = padding(sentence_token, labels)\n#    print(padding(sentence_token, training = 0 ))\n    \n    print(sentence_token) if info else 0\n#    if vectorized:\n#        flattened_token = torch.flatten(torch.tensor(sentence_token), start_dim = 0)    #Flattening to feature vectores of the size of 10000x1 (from 100x100) per sentence\n#        print(flattened_token.shape) if info else 0\n#        flattened_token = flattened_token.tolist()\n#    else:\n#        flattened_token = sentence_token\n    \n    return sentence_token\n    \n@autotest\ndef tokenize_article(article, vectorized = 1, labels = 0, autotest = 0, info = 0):\n    '''\n    Creates a list of lists of tokens of the article. The last sentence of the article retains a dot.\n    \n    Args: str from the dataset (a whole article), int (vectorization), int (training/labels)\n    \n    Returns: a list of lists containing str\n    \n    tokenize_article(\"First sentence. Second sentence.\", 0, autotest = 1)\n    [['First', 'sentence', '.', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], ['Second', 'sentence', '.', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n    '''\n    sentence_token_list = []\n    for sentence in get_sentences(article, autotest):\n#        if not autotest:\n        print(sentence) if info else 0\n#            print(sentence_token(sentence,vectorized, labels))\n        sentence_token_list += [sentence_token(sentence,vectorized, labels, autotest, pad = 1)]\n        print(sentence_token_list) if info else 0\n#    if not autotest:\n#        print(\"Article tokenized.\")\n#    print(torch.tensor(sentence_token_list).shape) if not info else 0\n    return sentence_token_list","metadata":{"execution":{"iopub.status.busy":"2022-09-02T15:38:52.895129Z","iopub.execute_input":"2022-09-02T15:38:52.896014Z","iopub.status.idle":"2022-09-02T15:38:53.039624Z","shell.execute_reply.started":"2022-09-02T15:38:52.89594Z","shell.execute_reply":"2022-09-02T15:38:53.038299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w2v_model.wv.key_to_index.keys();","metadata":{"execution":{"iopub.status.busy":"2022-09-02T15:38:53.041778Z","iopub.execute_input":"2022-09-02T15:38:53.042738Z","iopub.status.idle":"2022-09-02T15:38:53.050821Z","shell.execute_reply.started":"2022-09-02T15:38:53.042604Z","shell.execute_reply":"2022-09-02T15:38:53.049181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = nlp_df_texts   \ndef create_vocab(dataset, vectorized = 1, labels = 0, autotest = 0, info = 0):\n    '''\n    Split into sentences -> create sentence tokens -> apply word2vec to sentence tokens of the form: [[\"str\", \"str\", ..., \".\"], [\"str\", \"str\", ..., \".\"], ...]\n    \n    Args: str (entire dataset)\n    \n    Returns: None (updates the vocabulary)\n    '''\n    w2v_model = gensim.models.Word2Vec.load(\"/kaggle/working/word2vec.model\")\n    for article in dataset:\n        vocab_len = len(w2v_model.wv)\n#        print(\"Vocabulary count before training step: \" + \"{}\".format(vocab_len))\n        w2v_model = gensim.models.Word2Vec.load(\"/kaggle/working/word2vec.model\")\n    \n#        list_of_sentences = tokenize_article(article, 0)\n        sentences = get_sentences(article)\n        list_of_sentences = []\n        for sentence in sentences:\n            list_of_sentences += [sentence_token(sentence, vectorized = 0, labels = 0, autotest = 0, info = 0, pad = 0)]\n#        print(list_of_sentences)\n#        print(list_of_sentences)\n        w2v_model = train_dictionary(list_of_sentences)\n        w2v_model.save(\"/kaggle/working/word2vec.model\")\n        w2v_model = gensim.models.Word2Vec.load(\"/kaggle/working/word2vec.model\")\n        vocab_len = len(w2v_model.wv)\n        print(\"Vocabulary count after training step: \" + \"{}\".format(vocab_len), end = \"\\r\")\n#    w2v_model = KeyedVectors.load(\"word2vec.model\", mmap='r')    \n    print(w2v_model.wv.key_to_index.keys()) if info else 0\n    return w2v_model\ncreate_vocab(dataset, info = 0)  #Starts vocabulary training\nw2v_model = gensim.models.Word2Vec.load(\"/kaggle/working/word2vec.model\")\n#w2v_model.wv.key_to_index.keys()","metadata":{"execution":{"iopub.status.busy":"2022-09-02T15:38:53.053476Z","iopub.execute_input":"2022-09-02T15:38:53.053923Z","iopub.status.idle":"2022-09-02T15:39:19.627746Z","shell.execute_reply.started":"2022-09-02T15:38:53.053879Z","shell.execute_reply":"2022-09-02T15:39:19.625065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"string = \"simple computer. very simple\"\ntempsentences = get_sentences(string)\nprint(tempsentences)\nprint(tempsentences[0])\ntoks = sentence_token(tempsentences[0], vectorized = 1)\n#print(tokens)\n#print(tokenizer([\"simple\"]))\ntok = tokenize_article(string)\nprint(torch.tensor(toks).shape)\nprint(torch.tensor(tok).shape)\nt = torch.tensor(tok)    #Need to flatten sentences in vectorized form, so each sentence goes from 100 x 100 to 10000 x 1.\n#print(t.shape)\nflattened = torch.flatten(t, start_dim = 1)\nflattened.shape","metadata":{"execution":{"iopub.status.busy":"2022-09-02T15:39:19.629889Z","iopub.execute_input":"2022-09-02T15:39:19.63026Z","iopub.status.idle":"2022-09-02T15:39:19.689034Z","shell.execute_reply.started":"2022-09-02T15:39:19.630229Z","shell.execute_reply":"2022-09-02T15:39:19.687514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w2v_model = gensim.models.Word2Vec.load(\"/kaggle/working/word2vec.model\")\nw2v_model.wv.key_to_index.keys();","metadata":{"execution":{"iopub.status.busy":"2022-09-02T15:39:19.691092Z","iopub.execute_input":"2022-09-02T15:39:19.691756Z","iopub.status.idle":"2022-09-02T15:39:19.764058Z","shell.execute_reply.started":"2022-09-02T15:39:19.691708Z","shell.execute_reply":"2022-09-02T15:39:19.762697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LSTM() returns tuple of (tensor, (recurrent state))\nw2v_length = 100\nclass extract_tensor(nn.Module):\n    def forward(self,x):\n        # Output shape (batch, features, hidden)\n        tensor, _ = x\n        # Reshape shape (batch, hidden)\n        return tensor[:, -1, :]\n\n\n#RNN (LSTM)\ndataset = nlp_df_texts\nmodel = Word2Vec.load(\"word2vec.model\")\nclass RNN_LSTM(nn.Module):\n    def __init__(self, embedding_dim, hidden_dim, n_layers = 2, lr = 0.003, batches = 0):\n        super(RNN_LSTM, self).__init__()\n        self.embedding_dim = embedding_dim   #the dimensions of the input.\n        self.hidden_dim = hidden_dim   #Width of the model.\n        self.batches = batches    #Number of examples\n        self.num_layers = n_layers   #Depth of the model.\n        self.lr = lr   #Learning rate\n        self.h_0 = torch.nn.Parameter(torch.rand(self.num_layers * 2, self.batches, int(0.5 * np.floor(self.hidden_dim)), requires_grad = True))\n        self.c_0 = torch.nn.Parameter(torch.rand(self.num_layers * 2, self.batches, int(1 * np.floor(self.hidden_dim)), requires_grad = True))\n        print(self.embedding_dim, self.hidden_dim, self.num_layers, self.lr)\n        # Next line is for setting up the neural net, LSTM is the RNN variant (bidirectional).\n        self.modnlp = nn.Sequential(\n            nn.LSTM(input_size = w2v_length, hidden_size = self.hidden_dim, proj_size = int(np.floor(0.5 * self.hidden_dim)), num_layers = self.num_layers, batch_first = True, dropout = 0.3, bidirectional = True),\n            extract_tensor(),\n            nn.Linear(in_features = self.hidden_dim , out_features = int(np.floor(0.5 * self.embedding_dim)), bias = True , device = None , dtype = None ),\n            nn.Softmax(dim = 1)\n#            self.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n        )\n        \n    \n        \n    def load_article(self, article, vectorized = 1, labels = 0):   #Helper function.\n#        print(article)\n        loaded_article = tokenize_article(article, vectorized, labels)\n#        print(loaded_article)\n#        print(loaded_article)\n        self.max_sentence_length = lambda max_sentence_length: max(article.map(len))\n#        print(tokenize_article(article, vectorized, labels))\n        return loaded_article\n    \n    def load_all_articles(self, dataset, vectorized = 1):   #Loading the dataset into the object.\n        training_dataset = []\n        labels_dataset = []\n        index = 1\n        for article in dataset:\n            print(\"Article \" + f\"{index}\", end = \"\\r\")\n#            print(article)\n#            print(self.load_article(article, vectorized = 1, labels = 0))\n            training_dataset += self.load_article(article, vectorized = 1, labels = 0)\n            labels_dataset += self.load_article(article, vectorized = 1, labels = 1)\n#            print(torch.tensor(training_dataset).shape)\n            index += 1\n        print(torch.tensor(labels_dataset).shape)\n#        print(training_dataset)\n        no_batches = torch.tensor(training_dataset).shape[0]\n        print(no_batches)\n        return (training_dataset, labels_dataset, no_batches)\n    \n    def forward(self, training_dataset, labels_dataset, epochs = 5):   #Training step.\n        linear = nn.Linear(in_features = self.hidden_dim , out_features = int(np.floor(0.5 * self.embedding_dim)), bias = True , device = None , dtype = None )\n        lstm = nn.LSTM(input_size = w2v_length, hidden_size = self.hidden_dim, proj_size = int(np.floor(0.5 * self.hidden_dim)), num_layers = self.num_layers, batch_first = True, dropout = 0.3, bidirectional = True)\n        softmax = nn.Softmax(dim = 1)\n        training_dataset = torch.tensor(training_dataset)\n        labels_dataset = torch.tensor(labels_dataset)\n        print(training_dataset.shape)\n        print(type(training_dataset))\n        loss_function = nn.CrossEntropyLoss()\n        optimizer = torch.optim.Adam(self.parameters(), self.lr)\n#        print(self.parameters())\n        h_0 = self.h_0\n        c_0 = self.c_0\n        h_0.requires_grad_(True)\n        c_0.requires_grad_(True)\n        logits = torch.tensor(torch.zeros(1))\n        logits.requires_grad_(True)\n        self.train()\n        for epoch in range(epochs):\n            optimizer.zero_grad()\n            logits, (h_n, c_n) = lstm(training_dataset, (h_0, c_0))\n#            print(torch.tensor(logits).shape)\n            logits = linear(logits)\n            logits = softmax(logits)\n            logits = torch.tensor(logits).requires_grad_(True)\n#            print(torch.tensor(logits).shape)\n            logits_labels = linear(labels_dataset)\n            logits_labels = softmax(logits_labels)\n            logits_labels = torch.tensor(logits_labels)\n#            print(soft_logits[0] - soft_logits_labels[0])\n            print(logits.shape)\n            loss = loss_function(logits, logits_labels)\n            loss.requires_grad_(True)\n            loss.retain_grad()\n            loss.backward(retain_graph = True)\n            optimizer.step()\n            print(loss.item(), \"epoch\" + f\"{epoch}\")\n#            print(logits.grad)\n        self.eval()\n        return self.parameters, loss\n        #self.modnlp(training_dataset, labels_dataset)\n        \n#        print(self.model.summary())\n\n    def forward2(self, training_dataset, labels_dataset, epochs = 5):   #Training step.\n        training_dataset = torch.tensor(training_dataset)\n        labels_dataset = torch.tensor(labels_dataset)\n        print(training_dataset.shape)\n        print(type(training_dataset))\n        loss_function = nn.CrossEntropyLoss()\n        optimizer = torch.optim.Adam(self.parameters(), self.lr)\n#        print(self.parameters())\n        h_0 = self.h_0\n        c_0 = self.c_0\n        h_0.requires_grad_(True)\n        c_0.requires_grad_(True)\n        logits = torch.tensor(torch.zeros(1))\n        logits.requires_grad_(True)\n        self.train()\n        for epoch in range(epochs):\n            optimizer.zero_grad()\n            logits = self.modnlp(training_dataset).requires_grad_(True)\n#            print(torch.tensor(logits).shape)\n            logits_labels = self.modnlp(labels_dataset)\n#            print(soft_logits[0] - soft_logits_labels[0])\n            print(logits.shape)\n            loss = loss_function(logits, logits_labels)\n            loss.requires_grad_(True)\n            loss.retain_grad()\n            loss.backward(retain_graph = True)\n            optimizer.step()\n            print(loss.item(), \"epoch\" + f\"{epoch}\")\n#            print(logits.grad)\n        self.eval()\n        return self.parameters, loss","metadata":{"execution":{"iopub.status.busy":"2022-09-02T15:39:19.766408Z","iopub.execute_input":"2022-09-02T15:39:19.767198Z","iopub.status.idle":"2022-09-02T15:39:19.860158Z","shell.execute_reply.started":"2022-09-02T15:39:19.767125Z","shell.execute_reply":"2022-09-02T15:39:19.858803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(dataset)\n#test_article = tokenize_article(dataset[0], vectorized = 1, labels = 0);\n#test_article2 = test_article[0]\n#print(len(test_article[:][:][0][:]))\n#print(test_article[:][:][0][:])","metadata":{"execution":{"iopub.status.busy":"2022-09-02T15:39:19.862144Z","iopub.execute_input":"2022-09-02T15:39:19.863077Z","iopub.status.idle":"2022-09-02T15:39:19.872361Z","shell.execute_reply.started":"2022-09-02T15:39:19.863028Z","shell.execute_reply":"2022-09-02T15:39:19.870742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_dataset, labels_dataset, no_batches = RNN_LSTM(default_length, hidden_dim = w2v_length, n_layers = 2, lr = 0.003).load_all_articles(dataset)    #Loads the dataset.","metadata":{"execution":{"iopub.status.busy":"2022-09-02T15:39:19.87486Z","iopub.execute_input":"2022-09-02T15:39:19.876165Z","iopub.status.idle":"2022-09-02T15:40:04.668452Z","shell.execute_reply.started":"2022-09-02T15:39:19.876103Z","shell.execute_reply":"2022-09-02T15:40:04.666539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_generator = RNN_LSTM(default_length, hidden_dim = w2v_length, batches = no_batches, n_layers = 2, lr = 0.003).forward2(training_dataset, labels_dataset, epochs = 20)   #Runs the model.","metadata":{"execution":{"iopub.status.busy":"2022-09-02T15:40:04.67046Z","iopub.status.idle":"2022-09-02T15:40:04.671157Z","shell.execute_reply.started":"2022-09-02T15:40:04.67081Z","shell.execute_reply":"2022-09-02T15:40:04.670841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#if nlp_df:\n#    del nlp_df","metadata":{"execution":{"iopub.status.busy":"2022-09-02T15:40:04.673693Z","iopub.status.idle":"2022-09-02T15:40:04.674365Z","shell.execute_reply.started":"2022-09-02T15:40:04.674029Z","shell.execute_reply":"2022-09-02T15:40:04.67406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.tensor(training_dataset).shape","metadata":{"execution":{"iopub.status.busy":"2022-09-02T15:40:04.676981Z","iopub.status.idle":"2022-09-02T15:40:04.67778Z","shell.execute_reply.started":"2022-09-02T15:40:04.67743Z","shell.execute_reply":"2022-09-02T15:40:04.677462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}